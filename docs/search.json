[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "사회과학 연구자를 위한 파이썬 기반 통계 분석",
    "section": "",
    "text": "맞이하는 글\n사회과학 연구는 복잡다단한 사회 현상의 심층적 이해와 과학적 설명을 목표로 끊임없이 정진해 왔습니다. 급증하는 데이터의 양과 비약적으로 발전하는 정보 기술은 사회과학 연구의 새로운 패러다임을 제시하며, 그중에서도 파이썬은 고성능 데이터 분석 도구와 광범위한 라이브러리 생태계를 기반으로 사회과학 연구자에게 필수적인 분석 역량으로 부상하고 있습니다.\n본 저서는 인문 사회 분야 연구자들이 기존의 상용 통계 패키지의 제한적인 환경에서 벗어나, 파이썬을 활용하여 보다 유연하고 효율적인 분석 체계를 구축하고, 표준화된 코드를 통해 분석 전 과정을 체계적으로 관리하며, 시각화된 결과물을 논문 작성의 근거 자료로 활용하는 데 실질적인 도움을 제공하고자 기획되었습니다.\n기존의 통계학 교재들이 이론적 논의에 치중하거나, 특정 통계 패키지의 조작법만을 단편적으로 기술한 데 반하여, 본 저서는 가상 데이터를 기반으로 각 통계 분석 기법의 이론적 토대와 파이썬 라이브러리를 이용한 실제 분석 절차를 통합적으로 제시합니다. 독자들은 다양한 형태의 가상 데이터를 직접 조작하고, 파이썬 코드를 실행하여 도출된 결과를 해석하는 실습 과정을 통해 통계 분석 능력을 체계적으로 함양할 수 있습니다. 더불어, 각 분석 방법론이 사회과학 연구의 다양한 영역에서 어떻게 적용될 수 있는지 구체적인 사례를 제시함으로써 학습의 응용 가능성을 제고하였습니다.\n본 저서를 통해 독자들이 파이썬 기반의 통계 분석에 대한 확신을 얻고, 데이터 기반의 객관적이고 과학적인 사회과학 연구를 수행하며 학문적 성과의 완성도를 제고하는 데 필요한 핵심 역량을 확보하게 되기를 기대하는 바입니다.\n선행학습 : 파이썬 문법 기초 및 자료형",
    "crumbs": [
      "맞이하는 글"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "목차",
    "section": "",
    "text": "제1부: 파이썬 기반 사회과학 데이터 분석 준비\n\n제1장: 사회과학 연구와 데이터의 이해\n\n과학적 연구 방법론의 기초\n사회과학 데이터의 유형과 특성 (설문, 실험, 관찰 데이터 등)\n가상 데이터의 효용성과 장점\n파이썬 통계 분석 환경 구축 (Anaconda, Jupyter Notebook 소개)\n주요 파이썬 라이브러리 개요 (Pandas, NumPy, Matplotlib, Seaborn)\n\n제2장: 파이썬을 이용한 데이터 핸들링 및 탐색\n\nPandas DataFrame 기초: 데이터 로딩, 저장, 구조 이해\n데이터 전처리: 결측값 처리, 이상치 탐지 및 조정, 변수 변환\n기술 통계 분석: 평균, 표준편차, 빈도 분석 등\n데이터 시각화: Matplotlib, Seaborn 활용 (히스토그램, 산점도, 막대 그래프 등)\n가상 데이터 예시: 설문조사 응답 데이터 (개인 속성, 태도, 행위 관련 변수 포함)\n\n\n제2부: 집단 간 차이 검증 및 관계 분석\n\n제3장: 집단 간 평균 차이 검증\n\n독립표본 t-검정: 두 독립 집단의 평균 비교\n대응표본 t-검정: 동일 집단의 사전-사후 평균 비교\n일원분산분석 (ANOVA): 세 개 이상 집단의 평균 비교\n사후 검정 (Post-hoc test)\n가상 데이터 예시:\n\n상이한 교육 방식의 효과 비교 (학업 성취도 데이터)\n특정 사건 전후의 태도 변화 측정 데이터\n복수 지역 간 소득 수준 비교 데이터\n\n\n제4장: 범주형 데이터 분석 및 연관성 분석\n\n교차 분석 (Cross-tabulation): 범주형 변수 간 빈도 분석\n카이제곱 검정: 범주형 변수 간 독립성 검증\n피셔의 정확 검정\n가상 데이터 예시:\n\n성별과 특정 정치적 성향 간의 관계 분석 (설문 데이터)\n광고 노출 여부와 제품 구매 여부 간의 관계 분석\n\n\n제5장: 변수 간 상관관계 분석\n\n피어슨 상관분석: 연속형 변수 간 선형적 관계 측정\n스피어만 상관분석: 순위형 변수 간 관계 측정\n편상관분석: 통제 변수 고려 하 두 변수 간 순수 상관관계 분석\n가상 데이터 예시:\n\n소득 수준과 주관적 행복감 간의 관계 분석 (설문 데이터)\n광고 지출액과 매출액 간의 관계 분석 (시계열 또는 횡단면 데이터)\n\n\n\n제3부: 예측 및 설명 모형화\n\n제6장: 선형 회귀분석\n\n단순 선형 회귀분석: 단일 독립변수의 종속변수에 대한 영향 분석\n다중 선형 회귀분석: 복수 독립변수의 종속변수에 대한 영향 분석\n회귀 모형 평가 (결정 계수, F-통계량)\n회귀 진단 (잔차 분석, 다중공선성)\n가상 데이터 예시:\n\n광고비, 소득 수준의 제품 구매량에 대한 영향 분석\n학습 시간, 학습 방법의 시험 성적에 대한 영향 분석\n\n\n제7장: 로지스틱 회귀분석\n\n이항 로지스틱 회귀분석: 이분형 종속변수 예측\n다항 로지스틱 회귀분석: 다범주 종속변수 예측\n오즈비 (Odds Ratio) 해석\n모형 평가 (정확도, ROC 곡선)\n가상 데이터 예시:\n\n개인 속성이 특정 정치 후보 지지 여부에 미치는 영향 분석\n마케팅 캠페인 요소가 고객의 구매 전환 여부에 미치는 영향 분석\n\n\n제8장: 의사결정나무 모형\n\n분류 나무 (Classification Tree): 범주형 종속변수 예측\n회귀 나무 (Regression Tree): 연속형 종속변수 예측\n가지치기 (Pruning)\n변수 중요도 분석\n가상 데이터 예시:\n\n고객 특성에 따른 상품 추천 모형 개발\n학생 성적 예측 모형 개발\n\n\n제9장: 군집 분석\n\n비계층적 군집 분석 (K-평균 군집)\n계층적 군집 분석 (병합적, 분할적 방법)\n군집 평가\n가상 데이터 예시:\n\n소비자 행태 패턴 기반 시장 세분화\n지역별 사회 경제적 특성 기반 군집화\n\n\n제10장: 요인 분석\n\n탐색적 요인 분석: 잠재 변수 (요인) 추출\n확인적 요인 분석 (간략히 소개)\n요인 회전\n요인 점수\n가상 데이터 예시:\n\n소비자 브랜드 태도 측정 설문 데이터의 주요 요인 추출\n직무 만족도 설문 데이터의 하위 요인 구조 파악\n\n\n\n제4부: 심화 분석 기법 (선택적)\n\n제11장: 구조방정식 모형 (SEM) 개요\n\n경로 분석\n잠재 변수 모형\n모형 식별 및 적합도 평가 (개략적 소개)\n가상 데이터 예시: 사회적 지지와 우울증 간 관계에서 자존감의 매개 효과 분석 (가상 경로 모형 제시)\n\n제12장: 다차원 척도법 (MDS)\n\n객체 간 유사성/비유사성 데이터 분석\n저차원 공간에서의 객체 시각화\n가상 데이터 예시: 다양한 브랜드 간 이미지 유사성 평가 데이터 분석\n\n제13장: 상응 분석\n\n범주형 변수 간 관계 시각화\n행 및 열 프로필 분석\n가상 데이터 예시: 제품 범주와 소비자 특성 간 관계 분석\n\n제14장: 컨조인트 분석 (Conjoint Analysis)\n\n속성 수준 조합에 대한 선호도 분석\n각 속성 수준의 중요도 파악\n가상 데이터 예시: 신제품의 다양한 속성 조합에 대한 소비자 선호도 분석\n\n제15장: 시계열 분석 기초 (선택적)\n\n시계열 데이터의 이해\n추세 및 계절성 분석 (개략적 소개)\nARIMA 모형 기초 (개략적 소개)\n가상 데이터 예시: 월별 판매량 데이터 분석\n\n\n부록:\n\n주요 파이썬 라이브러리 함수 요약\n통계 용어 해설\n연습 문제 및 해답",
    "crumbs": [
      "목차"
    ]
  },
  {
    "objectID": "01-research-data.html",
    "href": "01-research-data.html",
    "title": "1  제1장: 사회과학 연구와 데이터의 이해",
    "section": "",
    "text": "1.1 과학적 연구 방법론의 기초\n사회과학 연구는 인간 사회의 다양한 측면을 과학적인 원리와 절차에 따라 탐구하여 체계적이고 신뢰할 수 있는 지식을 구축하고자 하는 학문적 노력입니다. 이러한 연구의 핵심에는 과학적 연구 방법론(Scientific Research Methodology)이 자리 잡고 있으며, 이는 우리가 사회 현상을 이해하고 설명하는 방식을 규정하는 중요한 틀을 제공합니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>제1장: 사회과학 연구와 데이터의 이해</span>"
    ]
  },
  {
    "objectID": "01-research-data.html#과학적-연구-방법론의-기초",
    "href": "01-research-data.html#과학적-연구-방법론의-기초",
    "title": "1  제1장: 사회과학 연구와 데이터의 이해",
    "section": "",
    "text": "1.1.1 과학적 방법의 핵심 원리\n과학적 연구는 몇 가지 기본적인 원리에 기반합니다. 이 원리들은 과학적 탐구의 방향을 제시하고, 비과학적인 접근 방식과 차별화되는 특징을 나타냅니다.\n1. 경험주의 (Empiricism): 관찰 가능한 증거를 통한 지식 획득\n경험주의는 실제 세계에 대한 체계적인 관찰과 감각적 경험을 통해 지식을 획득하는 것을 강조하는 철학적 입장입니다. 과학적 연구는 추상적인 이론이나 개인적인 직관에 의존하기보다는, 객관적으로 측정하고 기록할 수 있는 실증적 증거(Empirical Evidence)를 토대로 결론을 도출합니다.\n\n(시각 자료 예시: “경험주의적 지식 획득 과정”을 나타내는 간단한 흐름도. 관찰 → 측정 → 기록 → 분석 → 결론 의 순서)\n사회 현상에 대한 과학적 주장은 단순히 논리적인 주장이나 개인적인 의견만으로는 충분하지 않습니다. 예를 들어, “텔레비전 폭력 프로그램 시청이 아동의 공격성을 증가시킨다”는 주장은 실제 아동들의 텔레비전 시청 행태와 공격성 수준을 다양한 방법으로 측정하고 분석한 경험적 자료를 통해 뒷받침되어야 과학적인 근거를 확보할 수 있습니다.\n예시: 연구자는 실험 집단과 통제 집단으로 아동을 나누어 실험 집단에게 폭력적인 텔레비전 프로그램을 시청하게 한 후, 두 집단의 공격성 수준을 표준화된 행동 관찰 도구를 사용하여 측정하고 그 결과를 비교 분석합니다. 이 과정에서 얻어진 행동 관찰 데이터가 경험적 증거에 해당합니다.\n\n2. 객관성 (Objectivity): 연구자의 주관적 편견 배제\n객관성은 연구자의 개인적인 가치관, 선호, 기대, 또는 편견이 연구의 모든 단계(문제 제기, 가설 설정, 자료 수집, 분석, 결론 도출)에 영향을 미치지 않도록 엄격하게 통제하는 원리입니다. 과학적 연구는 사실(Facts)에 기반해야 하며, 연구자의 주관적인 해석이나 감정에 따라 결과가 왜곡되어서는 안 됩니다.\n\n(시각 자료 예시: “주관적인 해석 vs. 객관적인 관찰”을 대비시키는 그림. 한쪽에는 연구자가 자신의 믿음에 따라 현상을 해석하는 모습, 다른 쪽에는 표준화된 도구를 사용하여 현상을 측정하는 모습)\n연구자는 자신의 주관적인 입장을 끊임없이 인식하고, 이를 통제하기 위한 노력을 기울여야 합니다. 이를 위해 명확하게 정의된 개념, 표준화된 측정 도구, 그리고 통계적 분석과 같이 객관적인 절차와 방법을 사용하는 것이 중요합니다.\n예시: 특정 사회 정책의 효과를 연구하는 경우, 연구자의 개인적인 정치적 성향이 설문 문항 설계, 응답 해석, 또는 결과 보고에 영향을 미치지 않도록 주의해야 합니다. 객관적인 연구는 다양한 이해관계자의 관점을 고려하고, 편향되지 않은 방법론을 사용하여 데이터를 분석하며, 결과에 대한 균형 잡힌 해석을 제시합니다.\n\n3. 체계성 (Systematicity): 계획적이고 논리적인 연구 과정\n과학적 연구는 즉흥적이거나 임의적인 탐색이 아니라, 사전에 치밀하게 계획된 체계적인 절차에 따라 진행됩니다. 각 연구 단계는 명확한 목표를 가지고 있으며, 이전 단계의 결과와 논리적으로 연결되어 최종 결론에 이르기까지 일관성을 유지해야 합니다.\n\n(시각 자료 예시: “과학적 연구의 체계적인 단계”를 나타내는 순환형 다이어그램. 문제 제기 → 문헌 고찰 → 가설 설정 → 연구 설계 → 자료 수집 → 자료 분석 → 결론 도출 및 보고 의 단계가 순환적으로 연결된 모습)\n체계적인 연구는 연구 질문을 명확히 정의하는 것에서 시작하여, 그 질문에 답하기 위한 최적의 방법을 신중하게 선택하고, 계획된 절차에 따라 데이터를 수집하고 분석합니다. 각 단계의 결과는 다음 단계의 방향을 제시하며, 전체 연구 과정은 논리적인 흐름을 따라 진행됩니다.\n예시: “사회적 지지가 개인의 스트레스 해소에 미치는 영향”을 연구할 때, 연구자는 먼저 관련 이론을 검토하고, 사회적 지지와 스트레스 해소의 개념을 명확히 정의한 후, 연구 대상 선정, 자료 수집 방법(설문 조사, 사회적 지지 척도, 스트레스 척도), 분석 방법(상관분석, 회귀분석) 등을 포함하는 상세한 연구 설계를 수립합니다. 이후 설계에 따라 데이터를 수집하고 분석하여 결론을 도출하는 체계적인 과정을 따릅니다.\n\n4. 일반성 (Generality): 특정 사례를 넘어 보편적인 설명 추구\n과학적 연구의 중요한 목표 중 하나는 특정 개인, 집단, 또는 상황에 대한 제한적인 설명을 넘어, 더 넓은 범위의 유사한 현상에 적용될 수 있는 일반적인 원리, 법칙, 또는 이론을 발견하는 것입니다. 일반적인 설명력을 갖는 지식은 사회 현상을 더 깊이 이해하고 예측하는 데 기여합니다.\n\n(시각 자료 예시: “일반성의 확장”을 나타내는 그림. 특정 사례에 대한 연구 결과가 더 큰 집단이나 유사한 상황에 적용될 수 있음을 화살표로 나타내는 모습)\n연구 결과의 일반성을 확보하기 위해서는 연구 대상을 신중하게 선정하고, 통계적으로 대표성 있는 표본을 추출하는 것이 중요합니다. 또한, 다양한 맥락에서 연구 결과를 반복적으로 검증하여 그 적용 범위를 확인하는 노력이 필요합니다.\n예시: 특정 연령대의 스마트폰 중독 연구에서 밝혀진 원인이 다른 연령대나 다른 문화권의 스마트폰 중독 현상에도 유사하게 적용될 수 있다면, 해당 연구 결과는 높은 수준의 일반성을 갖는다고 평가할 수 있습니다.\n\n5. 간결성 (Parsimony): 경제적이고 명료한 이론 구성\n간결성의 원리, 또는 오컴의 면도날(Occam’s Razor)은 동일한 사회 현상을 설명할 수 있는 여러 경쟁적인 이론이 존재할 때, 가장 적은 수의 가정으로 이루어진 가장 단순하고 명료한 이론을 선호하는 원리입니다. 단순한 이론은 이해하기 쉽고, 검증 가능성이 높으며, 새로운 예측을 생성하고 다른 현상을 설명하는 데 더 유용할 수 있습니다.\n\n(시각 자료 예시: “복잡한 설명 vs. 간결한 설명”을 비교하는 그림. 복잡한 설명은 여러 개의 화살표와 상자로 연결되어 있고, 간결한 설명은 몇 개의 핵심 요소만으로 명확하게 연결된 모습)\n과학자는 현상을 설명하기 위해 불필요하게 많은 변수나 복잡한 관계를 가정하는 것을 피해야 합니다. 핵심적인 요인과 그들 간의 명확한 관계를 제시하는 간결한 이론이 과학적 설명으로서 더 큰 가치를 지닙니다.\n예시: 청소년 비행의 원인을 설명하는 다양한 이론 중에서, 가정하는 요인의 수가 적고 그 관계가 명확하게 제시된 이론이, 다른 조건이 동일하다면 더 선호됩니다.\n\n6. 검증 가능성 (Verifiability): 경험적 증거를 통한 반복적 확인\n과학적 주장은 경험적인 증거를 통해 반복적으로 검증 가능해야 합니다. 이는 다른 연구자들이 동일한 이론적 틀과 방법론을 사용하여 연구를 수행했을 때, 유사한 결과를 얻을 수 있어야 함을 의미합니다. 검증 가능성은 과학적 지식의 신뢰성과 객관성을 확보하는 데 필수적인 조건입니다.\n\n(시각 자료 예시: “검증 과정”을 나타내는 그림. 하나의 연구 결과가 다른 연구자들에 의해 반복적으로 확인되는 과정을 화살표로 나타내는 모습)\n검증 가능성을 확보하기 위해서는 연구 과정과 결과를 투명하게 공개하고, 연구 방법을 상세하게 기술하여 다른 연구자들이 이를 따라 할 수 있도록 해야 합니다. 또한, 연구 결과에 대한 비판적인 검토와 토론을 통해 지식의 신뢰성을 높이는 과정이 중요합니다.\n예시: 특정 사회 복지 프로그램의 효과를 평가하는 연구 결과가 발표되었을 때, 다른 연구자들이 동일한 프로그램과 유사한 연구 설계를 사용하여 연구를 반복했을 때 유사한 효과가 나타난다면, 해당 연구 결과의 검증 가능성이 높다고 할 수 있습니다.\n\n\n\n1.1.2 과학적 연구 방법의 순환적 성격\n과학적 연구는 일반적으로 문제 제기, 문헌 고찰, 가설 설정, 연구 설계, 자료 수집, 자료 분석, 결론 도출 및 보고의 단계를 따르지만, 이 과정은 선형적인 진행이라기보다는 순환적인 성격을 갖습니다. 연구의 결과는 기존 이론을 지지하거나 수정할 수 있으며, 때로는 새로운 연구 질문이나 탐구 방향을 제시하기도 합니다.\n\n(시각 자료 예시: “과학적 연구의 순환 과정”을 나타내는 다이어그램. 결론 도출 단계에서 다시 문제 제기 단계로 화살표가 이어지는 순환적인 흐름을 보여주는 모습)\n하나의 연구 프로젝트가 완결되더라도, 그 결과는 새로운 연구의 출발점이 될 수 있습니다. 기존 이론에 대한 비판적인 검토, 예상치 못한 발견, 또는 해결되지 않은 질문들은 후속 연구의 중요한 주제가 됩니다. 이러한 순환적인 과정을 통해 사회과학 지식은 점진적으로 발전하고 심화됩니다.\n예시: 청소년의 온라인 커뮤니티 활동이 사회성에 미치는 영향을 연구한 결과, 긍정적인 영향과 부정적인 영향이 동시에 발견되었다면, 후속 연구는 이러한 상반된 결과를 설명하기 위해 특정 조건이나 매개 변수를 탐색하는 방향으로 진행될 수 있습니다.\n\n결론적으로, 과학적 연구 방법론은 사회 현상을 이해하기 위한 강력한 도구이며, 경험주의, 객관성, 체계성, 일반성, 간결성, 그리고 검증 가능성과 같은 핵심 원리들을 바탕으로 지식을 탐구하고 구축합니다. 이러한 원리들을 이해하고 연구 과정에 적용하는 것은 사회과학 연구의 신뢰성과 타당성을 높이는 데 필수적입니다. 다음 섹션에서는 사회과학 연구에서 활용되는 다양한 형태의 데이터와 그 특징에 대해 자세히 살펴보겠습니다.\n알겠습니다. 교재의 1.2절 “사회과학 데이터의 유형과 특성” 부분을 교과서적인 어투로 더욱 자세하게 설명하고, 다양한 예시와 시각 자료 활용 방안을 제시하여 독자의 이해를 돕도록 하겠습니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>제1장: 사회과학 연구와 데이터의 이해</span>"
    ]
  },
  {
    "objectID": "01-research-data.html#사회과학-데이터의-유형과-특성",
    "href": "01-research-data.html#사회과학-데이터의-유형과-특성",
    "title": "1  제1장: 사회과학 연구와 데이터의 이해",
    "section": "1.2 1.2 사회과학 데이터의 유형과 특성",
    "text": "1.2 1.2 사회과학 데이터의 유형과 특성\n사회과학 연구는 인간의 행동, 사회적 관계, 문화, 제도 등 사회 현상의 다양한 측면을 탐구합니다. 이러한 연구 과정에서 수집되고 분석되는 데이터는 연구의 기초 자료로서, 연구 질문에 대한 답을 찾는 데 핵심적인 역할을 합니다. 사회과학 데이터는 그 형태, 내용, 그리고 수집 방법에 따라 다양하게 분류될 수 있으며, 각 유형의 데이터는 고유한 특성과 분석 방법을 요구합니다.\n\n1.2.1 1.2.1 데이터의 형태에 따른 분류\n사회과학 데이터는 크게 양적 데이터(Quantitative Data)와 질적 데이터(Qualitative Data)로 구분할 수 있습니다.\n1. 양적 데이터 (Quantitative Data): 수치로 표현되는 정보\n양적 데이터는 수치(Numerical Values)로 표현될 수 있는 데이터를 의미합니다. 이는 측정, 계수, 또는 순위 부여를 통해 얻어지며, 통계 분석을 통해 패턴, 관계, 차이 등을 파악하는 데 주로 활용됩니다.\n\n(시각 자료 예시: “양적 데이터의 예”를 보여주는 표 또는 그래프. 표에는 설문조사 응답, 실험 결과, 통계 자료 등이 나열되고, 그래프에는 막대 그래프, 꺾은선 그래프, 산점도 등이 제시됨)\n양적 데이터는 다음과 같은 특징을 가집니다.\n\n수치화 가능성: 데이터의 속성을 숫자로 나타낼 수 있습니다.\n측정 가능성: 표준화된 도구나 방법을 사용하여 데이터를 측정할 수 있습니다.\n통계 분석 용이성: 다양한 통계적 기법을 적용하여 데이터를 분석하고 해석할 수 있습니다.\n\n1.2.1.1 양적 데이터의 종류\n양적 데이터는 그 측정 수준에 따라 더욱 세분화될 수 있습니다. 측정 수준은 데이터 분석 방법 선택에 중요한 영향을 미칩니다.\n\n\n\n\n\n\n\n\n\n측정 수준\n설명\n예시\n주요 통계 분석\n\n\n\n\n명목 척도\n범주 구분\n성별, 종교, 거주 지역, 혈액형\n빈도 분석, 최빈값\n\n\n서열 척도\n순서 관계\n만족도, 선호도 순위, 사회경제적 계층\n중앙값, 분위수, 순위 상관\n\n\n등간 척도\n동일 간격, 영점 부재\n온도, 시간, 시험 점수\n평균, 표준편차, 상관\n\n\n비율 척도\n동일 간격, 절대 영점\n소득, 나이, 키, 몸무게\n평균, 표준편차, 비율 비교\n\n\n\n\n명목 척도(Nominal Scale):\n\n데이터를 상호 배타적인 범주(Category)로 구분하는 척도입니다.\n범주 간에는 순서나 크기의 의미가 없습니다.\n예시: 성별(남, 여), 종교(기독교, 불교, 이슬람교 등), 거주 지역(서울, 경기, 강원 등), 혈액형(A, B, AB, O)\n통계 분석: 빈도 분석, 최빈값, 카이제곱 검정 등\n\n서열 척도(Ordinal Scale):\n\n범주 간에 순서 관계(Order)는 존재하지만, 간격의 크기는 일정하지 않은 척도입니다.\n‘더 크다’ 또는 ’더 작다’와 같은 비교는 가능하지만, 그 차이가 얼마나 되는지는 알 수 없습니다.\n예시: 만족도(매우 불만족 &lt; 불만족 &lt; 보통 &lt; 만족 &lt; 매우 만족), 선호도 순위(1순위, 2순위, 3순위 등), 사회경제적 계층(하 &lt; 중 &lt; 상)\n통계 분석: 중앙값, 분위수, 순위 상관분석 등\n\n등간 척도(Interval Scale):\n\n범주 간에 순서 관계가 존재하고, 간격의 크기가 동일한 척도입니다.\n범주 간의 차이를 정확하게 측정할 수 있지만, 절대적인 영점(Absolute Zero)은 존재하지 않습니다.\n예시: 온도(섭씨, 화씨), 시간(기원전/기원후), 시험 점수(일부 논란 있음)\n통계 분석: 평균, 표준편차, 상관분석, 회귀분석 등\n\n비율 척도(Ratio Scale):\n\n등간 척도의 모든 특징을 가지면서 절대적인 영점이 존재하는 척도입니다.\n영점은 측정 속성이 완전히 ’없음’을 의미하며, 따라서 측정 값들의 비율 비교가 가능합니다.\n예시: 소득, 나이, 키, 몸무게, 반응 시간\n통계 분석: 평균, 표준편차, 비율 비교, 곱 상관분석, 회귀분석 등\n\n\n\n2. 질적 데이터 (Qualitative Data): 언어, 텍스트, 이미지 등\n질적 데이터는 언어(Text), 이미지(Image), 음성(Audio), 비디오(Video) 등 수치화하기 어려운 형태의 데이터를 의미합니다. 이는 심층적인 이해, 맥락 파악, 그리고 현상의 다양한 측면을 탐구하는 데 유용합니다.\n\n(시각 자료 예시: “질적 데이터의 예”를 보여주는 다양한 자료. 인터뷰 녹취록, 관찰 노트, 사진, 영상 캡처 화면 등이 제시됨)\n질적 데이터는 다음과 같은 특징을 가집니다.\n\n비수치적 정보: 데이터의 속성을 숫자로 나타내기 어렵습니다.\n맥락적 이해 중시: 데이터의 의미를 파악하기 위해 맥락을 고려하는 것이 중요합니다.\n심층적 분석: 데이터의 심층적인 의미와 다양한 측면을 탐구하는 데 활용됩니다.\n\n1.2.1.2 질적 데이터의 종류\n질적 데이터는 그 형태와 수집 방법에 따라 다양하게 분류될 수 있습니다.\n\n\n\n\n\n\n\n\n데이터 유형\n설명\n분석 방법\n\n\n\n\n텍스트 데이터\n인터뷰 녹취록, 문서 자료, 소셜 미디어 게시글\n텍스트 분석, 담화 분석\n\n\n이미지 데이터\n사진, 그림, 도표 등 시각 자료\n시각적 내용 분석\n\n\n음성 데이터\n인터뷰 녹음, 회의 녹음\n음성 분석\n\n\n비디오 데이터\n관찰 기록, 영상 자료 등 시간 정보\n영상 분석\n\n\n\n\n텍스트 데이터(Text Data):\n\n인터뷰 녹취록, 문서 자료, 소셜 미디어 게시글, 뉴스 기사 등 언어로 기록된 데이터입니다.\n텍스트 분석(Text Analysis), 담화 분석(Discourse Analysis) 등을 통해 분석됩니다.\n\n이미지 데이터(Image Data):\n\n사진, 그림, 도표 등 시각적인 정보를 담고 있는 데이터입니다.\n시각적 내용 분석(Visual Content Analysis) 등을 통해 분석됩니다.\n\n음성 데이터(Audio Data):\n\n인터뷰 녹음, 회의 녹음 등 음성 정보를 담고 있는 데이터입니다.\n음성 분석(Speech Analysis) 등을 통해 분석됩니다.\n\n비디오 데이터(Video Data):\n\n관찰 기록, 영상 자료 등 시간의 흐름에 따른 정보를 담고 있는 데이터입니다.\n영상 분석(Video Analysis) 등을 통해 분석됩니다.\n\n\n\n\n\n1.2.2 1.2.2 사회과학 데이터의 수집 방법\n사회과학 데이터는 다양한 방법을 통해 수집될 수 있으며, 각 방법은 데이터의 특성과 연구 목적에 따라 선택됩니다.\n\n(시각 자료 예시: “사회과학 데이터 수집 방법”을 나타내는 인포그래픽. 설문 조사, 실험, 관찰, 문헌 연구 등의 방법이 그림과 함께 제시됨)\n\n\n\n\n\n\n\n\n\n수집 방법\n설명\n장단점\n주요 활용 분야\n\n\n\n\n설문 조사\n질문지를 통해 연구 대상에게 정보 획득\n대규모 표본 정보 수집 용이, 구조화된 정보\n정치 여론 조사, 소비자 만족도 조사, 사회 의식 조사\n\n\n실험\n변수 조작을 통해 다른 변수에 미치는 영향 파악\n인과 관계 명확하게 규명\n광고 효과 실험, 교육 프로그램 효과 실험\n\n\n관찰\n연구 대상의 행동이나 현상 직접 관찰\n현장 맥락 이해, 심층 정보 획득 용이, 자연스러움\n조직 문화 연구, 소비자 행동 연구\n\n\n문헌 연구\n기존 문헌 자료(기록, 문서, 통계 자료) 분석\n역사적 분석, 정책 분석 등에 활용\n역사 연구, 정책 연구\n\n\n\n\n1. 설문 조사(Survey):\n\n연구 대상에게 질문지를 통해 정보를 얻는 방법입니다.\n구조화된 질문(Closed-ended questions)과 개방형 질문(Open-ended questions)을 모두 사용할 수 있습니다.\n대규모 표본을 대상으로 정보를 수집하는 데 효율적입니다.\n예시: 정치적 태도 조사, 소비자 만족도 조사, 사회 의식 조사\n\n2. 실험(Experiment):\n\n특정 변수를 조작하여 다른 변수에 미치는 영향을 알아보는 방법입니다.\n인과 관계(Causality)를 명확하게 밝히는 데 유용합니다.\n실험실 실험(Laboratory experiment)과 현장 실험(Field experiment)으로 구분됩니다.\n예시: 광고 효과 실험, 교육 프로그램 효과 실험, 사회적 영향력 실험\n\n3. 관찰(Observation):\n\n연구 대상의 행동이나 현상을 직접 관찰하여 정보를 얻는 방법입니다.\n참여 관찰(Participant observation)과 비참여 관찰(Non-participant observation)으로 구분됩니다.\n현장의 맥락을 이해하고 심층적인 정보를 얻는 데 유용합니다.\n예시: 조직 문화 연구, 소비자 행동 연구, 사회 운동 연구\n\n4. 문헌 연구(Document Study):\n\n기존의 문헌 자료(기록, 문서, 통계 자료 등)를 분석하여 정보를 얻는 방법입니다.\n역사 연구, 정책 연구, 내용 분석 등에 활용됩니다.\n예시: 역사적 사건 분석, 정책 변화 분석, 미디어 내용 분석\n\n\n\n1.2.3 1.2.3 사회과학 데이터의 윤리적 고려 사항\n사회과학 연구에서 데이터를 수집하고 분석하는 과정은 연구 대상의 권리와 이익을 보호하는 윤리적 책임을 수반합니다.\n\n(시각 자료 예시: “연구 윤리”를 강조하는 그림. 연구자와 연구 대상이 서로 존중하고 협력하는 모습)\n\n\n\n\n\n\n\n\n윤리적 원칙\n설명\n구체적 실천 방안\n\n\n\n\n자발적 참여\n연구 대상은 참여 여부를 자유롭게 결정\n강요나 압력 배제, 참여 철회 권리 보장\n\n\n사전 동의\n연구 목적, 절차, 위험, 이익 등에 대한 충분한 설명 후 동의\n서면 동의 획득, 미성년자/취약 계층의 경우 법적 대리인 동의\n\n\n익명성 보장\n연구 대상의 개인 정보 보호\n자료 분석 및 결과 보고 과정에서 신원 노출 방지\n\n\n비밀 보장\n연구 대상 정보의 비밀 유지\n연구 목적 외 사용 금지, 자료 보관 및 관리 보안 유지\n\n\n연구 결과 공정성\n연구 결과의 객관적이고 공정한 제시\n연구자 편견 배제, 연구 대상 집단에 대한 부정적 낙인/차별 조장 금지\n\n\n\n\n1. 연구 대상의 자발적 참여(Voluntary Participation):\n\n연구 대상은 연구 참여 여부를 자유롭게 결정할 권리가 있습니다.\n강요나 압력에 의한 참여는 윤리적으로 허용되지 않습니다.\n\n2. 사전 동의(Informed Consent):\n\n연구 대상은 연구의 목적, 절차, 위험, 이익 등에 대해 충분히 설명을 듣고, 연구 참여에 대한 동의를 서면으로 제공해야 합니다.\n미성년자나 취약 계층의 경우, 법적 대리인의 동의가 필요합니다.\n\n3. 익명성 보장(Anonymity):\n\n연구 대상의 개인 정보는 철저히 보호되어야 합니다.\n자료 분석 및 결과 보고 과정에서 연구 대상의 신원이 노출되지 않도록 주의해야 합니다.\n\n4. 비밀 보장(Confidentiality):\n\n연구 대상이 제공한 정보는 비밀로 유지되어야 하며, 연구 목적 외에 다른 용도로 사용되어서는 안 됩니다.\n자료 보관 및 관리 과정에서 보안을 철저히 유지해야 합니다.\n\n5. 연구 결과의 공정성(Fairness):\n\n연구 결과는 객관적이고 공정하게 제시되어야 하며, 연구자의 편견이나 이해관계에 따라 왜곡되어서는 안 됩니다.\n연구 대상 집단에 대한 부정적인 낙인이나 차별을 조장하는 내용은 포함되어서는 안 됩니다.\n\n\n\n1.2.4 1.2.4 사회과학 데이터의 중요성\n사회과학 데이터는 사회 현상을 이해하고 설명하며, 사회 문제를 해결하고 사회 정책을 수립하는 데 중요한 역할을 합니다. 데이터 기반의 연구는 객관적이고 신뢰할 수 있는 정보를 제공함으로써, 사회과학 지식의 발전에 기여하고, 더 나은 사회를 만들어가는 데 도움을 줄 수 있습니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>제1장: 사회과학 연구와 데이터의 이해</span>"
    ]
  },
  {
    "objectID": "01-research-data.html#가상-데이터의-효용성과-장점",
    "href": "01-research-data.html#가상-데이터의-효용성과-장점",
    "title": "1  제1장: 사회과학 연구와 데이터의 이해",
    "section": "1.3 1.3 가상 데이터의 효용성과 장점",
    "text": "1.3 1.3 가상 데이터의 효용성과 장점\n사회과학 연구에서 실제 데이터를 수집하고 분석하는 것은 시간, 비용, 윤리적 제약 등 다양한 어려움을 수반할 수 있습니다. 이러한 문제를 해결하고, 연구 및 교육 목적을 달성하기 위해 가상 데이터(Simulated Data)가 활용될 수 있습니다. 가상 데이터는 실제 데이터를 모방하여 인위적으로 생성된 데이터로서, 특정 통계적 속성을 갖도록 설계될 수 있습니다.\n\n1.3.1 1.3.1 가상 데이터의 정의 및 유형\n가상 데이터는 실제 데이터의 통계적 특성(평균, 표준편차, 분포, 변수 간 관계 등)을 유지하면서 인위적으로 생성된 데이터입니다. 이는 연구 및 교육 과정에서 실제 데이터를 대신하여 활용될 수 있으며, 다양한 유형으로 생성될 수 있습니다.\n\n(시각 자료 예시: “가상 데이터 생성 과정”을 나타내는 흐름도. 실제 데이터 분석 → 가상 데이터 생성 알고리즘 설계 → 가상 데이터 생성 → 가상 데이터 검증의 순서)\n가상 데이터는 생성 목적과 방법에 따라 다양한 유형으로 분류될 수 있습니다.\n\n모의 실험 데이터(Simulated Experimental Data):\n\n실험 연구의 결과를 모방하여 생성된 데이터입니다.\n실험 조건, 처리 효과, 오차 등을 제어하여 생성할 수 있습니다.\n예시: 약물 효과 실험 데이터, 교육 프로그램 효과 실험 데이터\n\n설문 조사 데이터(Simulated Survey Data):\n\n설문 조사의 응답 패턴을 모방하여 생성된 데이터입니다.\n응답자의 특성, 태도, 행동 등을 반영하여 생성할 수 있습니다.\n예시: 정치적 태도 조사 데이터, 소비자 만족도 조사 데이터\n\n시계열 데이터(Simulated Time Series Data):\n\n시간의 흐름에 따라 변화하는 데이터를 모방하여 생성된 데이터입니다.\n추세, 계절성, 주기성 등을 반영하여 생성할 수 있습니다.\n예시: 경제 성장률 데이터, 주가 변동 데이터\n\n\n\n\n\n1.3.2 1.3.2 가상 데이터의 효용성\n가상 데이터는 사회과학 연구 및 교육의 다양한 측면에서 유용하게 활용될 수 있습니다.\n\n(시각 자료 예시: “가상 데이터의 활용”을 보여주는 다양한 상황. 통계 분석 교육, 연구 방법론 실습, 윤리적 제약 극복 등이 제시됨)\n\n\n\n\n\n\n\n\n효용성\n설명\n구체적 예시\n\n\n\n\n접근 용이성\n실제 데이터 획득의 시간/비용 절감\n통계 분석 교육 자료 제공\n\n\n통제 가능성\n특정 분석 방법 설명/강조 용이\n회귀 분석 설명 위한 이상치 없는 데이터 생성\n\n\n반복 학습\n다양한 분석 방법 적용/결과 비교 용이\n군집 분석 방법 비교 위한 동일 데이터 활용\n\n\n개인 정보 보호\n민감한 개인 정보 유출 위험 없이 안전한 실습\n개인 정보 포함된 설문 조사 데이터 대신 활용\n\n\n윤리적 문제 해결\n윤리적 제약 있는 연구 주제 탐구 가능\n가상 실험 데이터 활용한 비윤리적 실험 재현 방지\n\n\n\n\n접근 용이성(Accessibility):\n\n실제 데이터를 획득하는 데는 시간, 비용, 그리고 노력이 요구될 수 있습니다.\n가상 데이터는 필요에 따라 쉽게 생성하고 활용할 수 있어, 연구 및 교육 자료로 활용하기에 용이합니다.\n예시: 통계 분석 교육 과정에서 다양한 예제 데이터를 제공하여 학습 효과를 높일 수 있습니다.\n\n통제 가능성(Controllability):\n\n가상 데이터는 특정 분석 방법을 설명하거나, 특정 통계적 개념을 강조하기 위해 데이터의 특성을 의도적으로 설계하고 통제할 수 있습니다.\n예시: 회귀 분석을 설명하기 위해 이상치가 없는 가상 데이터를 생성하여 모델의 적합도를 명확하게 보여줄 수 있습니다.\n\n반복 학습(Ease of Repetition):\n\n가상 데이터는 동일한 데이터를 반복적으로 분석하고, 다양한 분석 방법을 적용해 보면서 결과를 비교하고 이해하는 데 유용합니다.\n예시: 군집 분석 방법의 차이를 비교하기 위해 동일한 가상 데이터를 사용하여 각 방법의 결과를 시각적으로 비교할 수 있습니다.\n\n개인 정보 보호(Privacy Protection):\n\n실제 연구 데이터는 개인의 민감한 정보를 포함할 수 있으며, 데이터 공유 및 분석 과정에서 개인 정보 유출의 위험이 존재합니다.\n가상 데이터는 개인 정보를 포함하지 않으므로, 안전하게 분석 실습을 진행할 수 있습니다.\n예시: 개인의 소비 습관, 건강 정보 등이 포함된 설문 조사 데이터를 대신하여 가상 데이터를 활용하여 데이터 분석 기법을 학습할 수 있습니다.\n\n윤리적 문제 해결(Ethical Issue Resolution):\n\n일부 사회과학 연구는 윤리적인 문제로 인해 실제 데이터를 수집하기 어려운 경우가 있습니다.\n가상 데이터를 활용하면 이러한 윤리적 제약 없이 연구 주제를 탐구할 수 있습니다.\n예시: 인간에게 심각한 고통을 줄 수 있는 실험 연구는 가상 데이터를 활용하여 모의 실험을 진행함으로써 윤리적 문제를 해결할 수 있습니다.\n\n\n\n\n\n1.3.3 1.3.3 가상 데이터 활용 시 고려 사항\n가상 데이터는 다양한 장점을 제공하지만, 활용 시 몇 가지 고려 사항을 염두에 두어야 합니다.\n\n(시각 자료 예시: “가상 데이터 활용 시 주의사항”을 강조하는 그림. 가상 데이터의 한계점을 인식하고, 적절한 활용 방안을 모색하는 연구자의 모습)\n\n\n\n\n\n\n\n\n고려 사항\n설명\n구체적 예시\n\n\n\n\n현실 반영성\n실제 데이터의 복잡성/다양성 반영 어려움\n실제 데이터와 가상 데이터 간 분석 결과 차이 발생 가능성\n\n\n일반화 제한\n가상 데이터 분석 결과의 일반화에 주의\n특정 목적 위해 생성된 데이터는 일반적 결론 도출 어려움\n\n\n데이터 생성 과정 명확화\n데이터 생성 과정 및 가정 명확히 제시\n가상 데이터 활용 연구 결과 해석 시 주의\n\n\n\n\n현실 반영성(Reality Reflection):\n\n가상 데이터는 실제 데이터의 복잡성과 다양성을 완벽하게 반영하기 어려울 수 있습니다.\n가상 데이터 분석 결과는 실제 데이터 분석 결과와 차이가 있을 수 있다는 점을 염두에 두어야 합니다.\n예시: 실제 경제 데이터는 다양한 외부 요인에 의해 영향을 받지만, 가상 경제 데이터는 이러한 복잡성을 모두 반영하기 어려울 수 있습니다.\n\n일반화 제한(Generalization Limitation):\n\n가상 데이터는 특정 목적을 위해 생성되므로, 분석 결과를 실제 사회 현상에 일반화하는 데 주의가 필요합니다.\n가상 데이터 분석 결과는 특정 상황에 국한될 수 있으며, 일반적인 결론을 도출하기 어려울 수 있습니다.\n예시: 특정 마케팅 전략의 효과를 분석하기 위해 생성된 가상 데이터는 다른 상황에서의 마케팅 효과를 예측하는 데 한계가 있을 수 있습니다.\n\n데이터 생성 과정 명확화(Data Generation Process Clarification):\n\n가상 데이터의 생성 과정과 가정은 명확하게 제시되어야 합니다.\n가상 데이터 활용 연구 결과를 해석할 때, 데이터 생성 과정의 한계점을 고려해야 합니다.\n예시: 가상 데이터의 분포, 변수 간 관계, 이상치 생성 여부 등을 명확하게 밝혀, 연구 결과 해석의 타당성을 높여야 합니다.\n\n\n\n\n\n1.3.4 1.3.4 가상 데이터 활용의 미래\n가상 데이터는 사회과학 연구 및 교육에서 점점 더 중요한 역할을 할 것으로 예상됩니다. 인공지능, 머신러닝 등 기술 발전에 따라 더욱 정교하고 현실적인 가상 데이터 생성이 가능해짐에 따라, 가상 데이터 활용의 범위는 더욱 확대될 것입니다.\n\n(시각 자료 예시: “가상 데이터 활용의 미래”를 보여주는 그림. 인공지능 기술을 활용하여 더욱 현실적인 가상 데이터를 생성하고, 이를 사회과학 연구에 적용하는 미래 사회의 모습)\n\n인공지능 기반 가상 데이터 생성:\n\n인공지능, 머신러닝 기술을 활용하여 실제 데이터의 복잡한 패턴과 구조를 학습하고, 이를 반영한 더욱 현실적인 가상 데이터를 생성할 수 있습니다.\n\n가상 현실(VR)/증강 현실(AR) 연계:\n\n가상 현실, 증강 현실 기술과 연계하여 몰입형 데이터 환경을 구축하고, 사회 현상을 더욱 생생하게 모의 실험할 수 있습니다.\n\n윤리적 문제 해결 및 연구 혁신:\n\n가상 데이터 활용은 윤리적 문제 해결을 넘어, 사회과학 연구 방법론의 혁신을 가져올 수 있습니다.\n\n\n\n결론적으로, 가상 데이터는 사회과학 연구 및 교육의 효율성과 효과성을 높이는 데 기여할 수 있는 유용한 도구입니다. 가상 데이터의 효용성과 한계점을 명확히 인식하고, 적절한 활용 방안을 모색함으로써, 사회과학 연구의 발전에 기여할 수 있을 것입니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>제1장: 사회과학 연구와 데이터의 이해</span>"
    ]
  },
  {
    "objectID": "01-research-data.html#파이썬-통계-분석-환경-구축",
    "href": "01-research-data.html#파이썬-통계-분석-환경-구축",
    "title": "1  제1장: 사회과학 연구와 데이터의 이해",
    "section": "1.4 1.4 파이썬 통계 분석 환경 구축",
    "text": "1.4 1.4 파이썬 통계 분석 환경 구축\n파이썬(Python)은 강력한 데이터 분석 및 통계 분석 기능을 제공하는 프로그래밍 언어입니다. 다양한 라이브러리(Library)를 통해 통계 분석, 데이터 처리, 시각화 등 사회과학 연구에 필요한 다양한 기능을 지원합니다. 본 절에서는 파이썬을 활용한 효율적인 통계 분석 환경을 구축하는 데 필요한 주요 도구와 플랫폼을 소개합니다.\n\n1.4.1 1.4.1 파이썬 및 주요 라이브러리\n파이썬은 간결한 문법과 풍부한 라이브러리 생태계를 바탕으로, 데이터 과학 및 통계 분석 분야에서 널리 활용되고 있습니다.\n\n(시각 자료 예시: “파이썬 데이터 분석 생태계”를 보여주는 그림. 파이썬 로고와 함께 주요 라이브러리(Pandas, NumPy, Matplotlib, Seaborn 등)의 관계를 나타내는 그림)\n파이썬의 주요 라이브러리는 다음과 같습니다.\n\nPandas:\n\n구조화된 데이터(Structured Data), 특히 테이블 형태의 데이터를 효율적으로 처리하고 분석하기 위한 라이브러리입니다.\nDataFrame이라는 강력한 2차원 데이터 구조를 제공하며, 데이터 불러오기, 저장하기, 필터링, 정렬, 병합, 그룹화, 결측치 처리, 변수 변환 등 다양한 데이터 전처리 및 조작 기능을 제공합니다.\n\nNumPy(Numerical Python):\n\n파이썬에서 수치 계산(Numerical Computation)을 위한 핵심 라이브러리입니다.\n다차원 배열 객체인 ndarray를 제공하며, 배열 기반의 효율적인 수학 함수, 선형대수 연산, 통계 함수 등을 지원합니다.\n\nSciPy(Scientific Python):\n\n과학 및 기술 계산을 위한 광범위한 기능을 제공하는 라이브러리입니다.\n통계 검정(Statistical Tests), 최적화(Optimization), 선형대수(Linear Algebra), 신호 처리(Signal Processing), 보간(Interpolation), 미적분(Integration) 등 다양한 과학 기술 분야에서 활용되는 함수들을 포함하고 있습니다.\n\nStatsmodels:\n\n통계 모델링을 위한 라이브러리입니다.\n회귀 분석, 분산 분석, 시계열 분석 등 다양한 통계 모델을 구축하고 평가하는 기능을 제공합니다.\n\nMatplotlib:\n\n다양한 형태의 그래프와 플롯을 생성하여 데이터를 시각화하기 위한 라이브러리입니다.\n데이터의 패턴을 파악하고 분석 결과를 효과적으로 전달하는 데 도움을 줍니다.\n\nSeaborn:\n\nMatplotlib을 기반으로 통계적인 시각화 기능을 향상시킨 라이브러리입니다.\n더 세련되고 정보 전달력이 높은 그래프를 쉽게 생성할 수 있도록 지원합니다.\n\nScikit-learn:\n\n머신러닝을 위한 포괄적인 라이브러리입니다.\n분류, 회귀, 군집 분석, 차원 축소 등 다양한 머신러닝 알고리즘과 데이터 전처리, 모델 평가 도구를 제공합니다.\n\n\n\n\n\n\n\n\n\n\n라이브러리\n설명\n주요 기능\n\n\n\n\nPandas\n구조화된 데이터 처리 및 분석\nDataFrame, 데이터 전처리, 데이터 조작\n\n\nNumPy\n수치 계산 및 배열 연산\nndarray, 수학 함수, 선형대수, 통계 함수\n\n\nSciPy\n과학 및 기술 계산 함수 제공\n통계 검정, 최적화, 선형대수, 신호 처리, 미적분\n\n\nStatsmodels\n통계 모델링\n회귀 분석, 분산 분석, 시계열 분석\n\n\nMatplotlib\n데이터 시각화\n다양한 그래프 및 플롯 생성\n\n\nSeaborn\n통계적 데이터 시각화 기능 향상\n세련되고 정보 전달력 높은 그래프 생성\n\n\nScikit-learn\n머신러닝\n분류, 회귀, 군집 분석, 차원 축소, 모델 평가\n\n\n\n\n\n\n1.4.2 1.4.2 파이썬 통계 분석 환경 구축 도구\n파이썬을 활용한 통계 분석을 위해서는 효율적인 개발 환경을 구축하는 것이 중요합니다.\n\n(시각 자료 예시: “파이썬 통계 분석 환경 구축 도구”를 소개하는 그림. Anaconda, Jupyter Notebook, Google Colab 등의 로고와 함께 각 도구의 특징을 설명하는 그림)\n주요 환경 구축 도구는 다음과 같습니다.\n\nAnaconda:\n\n데이터 과학, 머신러닝, 그리고 과학 기술 컴퓨팅을 위한 오픈 소스 플랫폼입니다.\n파이썬 인터프리터와 함께 NumPy, Pandas, Matplotlib, SciPy, Jupyter Notebook 등 데이터 분석에 필수적인 다수의 패키지를 통합적으로 제공하므로, 개별적인 패키지 설치의 번거로움을 줄여줍니다.\n가상 환경(Virtual Environment) 관리 기능을 통해 프로젝트별로 독립적인 패키지 버전을 관리할 수 있도록 지원합니다.\n\nJupyter Notebook(또는 JupyterLab):\n\n웹 브라우저 기반의 대화형 컴퓨팅 환경(Interactive Computing Environment)입니다.\n코드 작성 및 실행, 텍스트 설명, 수식 입력, 시각화 결과 등을 하나의 문서 형태로 통합하여 관리할 수 있어, 데이터 분석 과정 기록, 학습 자료 제작, 연구 결과 공유 등에 매우 효과적입니다.\nJupyterLab은 Jupyter Notebook의 차세대 버전으로, 향상된 사용자 인터페이스와 추가 기능을 제공합니다.\n\nGoogle Colaboratory(Colab):\n\nGoogle에서 제공하는 클라우드 기반의 Jupyter Notebook 환경입니다.\n별도의 설치 없이 웹 브라우저만으로 파이썬 코드를 작성하고 실행할 수 있으며, GPU 및 TPU와 같은 하드웨어 자원을 무료로 활용할 수 있다는 장점이 있습니다.\n\n\n\n\n\n\n\n\n\n\n도구\n설명\n주요 특징\n\n\n\n\nAnaconda\n데이터 과학 플랫폼, 패키지 관리 및 가상 환경 지원\n파이썬, 주요 라이브러리, Jupyter Notebook 통합 제공\n\n\nJupyter Notebook\n웹 기반 대화형 컴퓨팅 환경\n코드, 텍스트, 수식, 시각화 결과 통합 관리\n\n\nGoogle Colaboratory\n클라우드 기반 Jupyter Notebook\n별도 설치 불필요, GPU/TPU 지원\n\n\n\n\n\n\n1.4.3 1.4.3 파이썬 통계 분석 환경 구축 절차\n효율적인 파이썬 통계 분석 환경을 구축하기 위한 일반적인 절차는 다음과 같습니다.\n\n(시각 자료 예시: “파이썬 통계 분석 환경 구축 절차”를 나타내는 순서도. Anaconda 설치 → Jupyter Notebook 실행 → 라이브러리 불러오기 → 데이터 분석 수행 → 결과 확인의 순서)\n\nAnaconda 설치:\n\nAnaconda 공식 웹사이트(https://www.anaconda.com/)에서 운영체제에 맞는 버전을 다운로드하여 설치합니다.\nAnaconda를 설치하면 파이썬 인터프리터와 함께 주요 데이터 분석 라이브러리, Jupyter Notebook 등이 자동으로 설치됩니다.\n\nJupyter Notebook 실행:\n\nAnaconda Navigator 또는 명령 프롬프트/터미널에서 jupyter notebook 또는 jupyter lab 명령어를 실행하여 Jupyter Notebook 또는 JupyterLab을 실행합니다.\n웹 브라우저가 자동으로 열리고, 파일 탐색기 또는 실행 화면이 표시됩니다.\n\n새로운 Notebook 생성:\n\nJupyter Notebook 또는 JupyterLab 화면에서 “New” 버튼을 클릭하고, “Python 3 (ipykernel)” 또는 유사한 옵션을 선택하여 새로운 Notebook 파일을 생성합니다.\n\n라이브러리 불러오기:\n\nNotebook의 코드 셀(Code Cell)에서 import 문을 사용하여 필요한 라이브러리를 불러옵니다.\n예시: import pandas as pd, import numpy as np, import matplotlib.pyplot as plt\n\n데이터 분석 수행:\n\nNotebook의 코드 셀에서 파이썬 코드를 작성하여 데이터를 불러오고, 전처리하고, 분석하고, 시각화합니다.\n코드 셀을 실행하려면 Shift + Enter 키를 누릅니다.\n\n결과 확인 및 문서화:\n\n코드 실행 결과는 코드 셀 바로 아래에 표시됩니다.\n텍스트 셀(Markdown Cell)을 사용하여 분석 과정, 결과 해석, 결론 등을 문서화합니다.\n\n\n\n\n\n1.4.4 1.4.4 파이썬 통계 분석 환경 활용 팁\n파이썬 통계 분석 환경을 더욱 효율적으로 활용하기 위한 몇 가지 팁은 다음과 같습니다.\n\n(시각 자료 예시: “파이썬 통계 분석 환경 활용 팁”을 제시하는 그림. 코드 재사용, 주석 활용, 시각화 활용 등을 강조하는 그림)\n\n코드 재사용(Code Reusability):\n\n자주 사용하는 코드는 함수(Function) 또는 클래스(Class)로 정의하여 재사용성을 높입니다.\n\n주석 활용(Comment Utilization):\n\n코드의 목적, 기능, 작동 방식 등을 주석으로 상세하게 설명하여 코드의 가독성을 높입니다.\n\n시각화 활용(Visualization Utilization):\n\n다양한 그래프와 플롯을 활용하여 데이터의 패턴을 파악하고, 분석 결과를 효과적으로 전달합니다.\n\n온라인 자료 활용(Online Resource Utilization):\n\n파이썬 및 라이브러리 관련 온라인 자료(공식 문서, 튜토리얼, 커뮤니티 등)를 적극적으로 활용하여 문제 해결 능력을 향상시킵니다.\n\n가상 환경 활용(Virtual Environment Utilization):\n\n프로젝트별로 독립적인 가상 환경을 생성하여 패키지 버전 충돌을 방지하고, 프로젝트의 독립성을 확보합니다.\n\n\n\n\n\n1.4.5 1.4.5 파이썬 통계 분석 환경의 장점\n파이썬 통계 분석 환경은 다음과 같은 장점을 제공합니다.\n\n(시각 자료 예시: “파이썬 통계 분석 환경의 장점”을 보여주는 그림. 유연성, 확장성, 생산성, 접근성 등을 강조하는 그림)\n\n유연성(Flexibility):\n\n다양한 라이브러리를 통해 통계 분석뿐만 아니라 데이터 처리, 시각화, 머신러닝 등 다양한 작업을 수행할 수 있습니다.\n\n확장성(Extensibility):\n\n사용자 정의 함수 및 클래스를 통해 기능을 확장하고, 특정 분석 목적에 맞는 도구를 개발할 수 있습니다.\n\n생산성(Productivity):\n\n간결하고 가독성이 높은 문법을 통해 코드 작성 시간을 단축하고, 개발 효율성을 높일 수 있습니다.\n\n접근성(Accessibility):\n\n오픈 소스이므로 무료로 사용할 수 있으며, 다양한 운영체제에서 동일하게 작동합니다.\n\n\n\n결론적으로, 파이썬은 강력하고 유연한 통계 분석 도구이며, Anaconda, Jupyter Notebook, Google Colab 등 다양한 도구를 통해 효율적인 개발 환경을 구축할 수 있습니다. 파이썬 및 관련 라이브러리의 기능을 익히고, 효과적인 개발 환경을 활용함으로써, 사회과학 연구자는 데이터 분석의 생산성과 효율성을 크게 향상시킬 수 있습니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>제1장: 사회과학 연구와 데이터의 이해</span>"
    ]
  },
  {
    "objectID": "02-data-handling-exploration.html",
    "href": "02-data-handling-exploration.html",
    "title": "2  파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초",
    "section": "",
    "text": "2.1 2.1 파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초\n사회과학 연구에서 데이터를 수집하고 분석하는 과정은 종종 복잡하고 방대한 양의 데이터를 다루는 것을 포함합니다. 파이썬의 Pandas 라이브러리는 이러한 데이터를 효율적으로 처리하고 분석하는 데 필수적인 도구입니다. 본 절에서는 Pandas의 핵심 자료 구조인 DataFrame을 중심으로, 데이터 핸들링 및 탐색의 기초를 다룹니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초</span>"
    ]
  },
  {
    "objectID": "02-data-handling-exploration.html#파이썬을-이용한-데이터-핸들링-및-탐색-pandas-dataframe-기초",
    "href": "02-data-handling-exploration.html#파이썬을-이용한-데이터-핸들링-및-탐색-pandas-dataframe-기초",
    "title": "2  파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초",
    "section": "",
    "text": "(시각 자료 예시: Pandas DataFrame의 구조를 보여주는 그림. 행과 열, 열 이름, 인덱스 등을 명확하게 표시한 표 형태의 그림)\n\n\n2.1.1 2.1.1 Pandas DataFrame의 개념\nPandas DataFrame은 2차원 테이블 형태의 데이터 구조로서, 행(Row)과 열(Column)로 구성됩니다. 각 열은 서로 다른 데이터 유형(숫자, 문자열, 날짜 등)을 가질 수 있으며, 스프레드시트나 SQL 테이블과 유사한 구조를 갖습니다.\nDataFrame은 다음과 같은 특징을 가집니다.\n\n다양한 데이터 유형 처리: 각 열은 서로 다른 데이터 유형을 가질 수 있습니다.\n레이블 기반 인덱싱: 행과 열에 레이블(이름)을 사용하여 데이터에 접근할 수 있습니다.\n결측치 처리: 결측치(Missing Data)를 효율적으로 처리하는 기능을 제공합니다.\n데이터 조작 기능: 데이터 필터링, 정렬, 그룹화, 병합 등 다양한 데이터 조작 기능을 제공합니다.\n\n\n\n2.1.2 2.1.2 DataFrame 생성\nDataFrame은 다양한 방법으로 생성할 수 있습니다.\n\n(시각 자료 예시: DataFrame 생성 방법을 보여주는 그림. 리스트, 딕셔너리, CSV 파일 등 다양한 입력 소스로부터 DataFrame을 생성하는 과정을 간략하게 보여주는 그림)\n리스트(List)로부터 생성:\n\n2차원 리스트를 사용하여 DataFrame을 생성할 수 있습니다.\n각 내부 리스트는 DataFrame의 한 행을 나타냅니다.\n\n\n\n    import pandas as pd\n\n    data = [['Alice', 25, 'Female'], ['Bob', 30, 'Male'], ['Charlie', 28, 'Male']]\n    df = pd.DataFrame(data, columns=['Name', 'Age', 'Gender'])\n    print(df)\n\n      Name  Age  Gender\n0    Alice   25  Female\n1      Bob   30    Male\n2  Charlie   28    Male\n\n\n\n딕셔너리(Dictionary)로부터 생성:\n\n딕셔너리를 사용하여 DataFrame을 생성할 수 있습니다.\n딕셔너리의 키(Key)는 DataFrame의 열 이름이 되고, 값(Value)은 열 데이터가 됩니다.\n\n\n\n    data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 28], 'Gender': ['Female', 'Male', 'Male']}\n    df = pd.DataFrame(data)\n    print(df)\n    df.to_csv('data.csv', index=False) # DataFrame을 data.csv 파일로 저장 (index 제외)\n\n      Name  Age  Gender\n0    Alice   25  Female\n1      Bob   30    Male\n2  Charlie   28    Male\n\n\n\nCSV 파일로부터 생성:\n\nCSV(Comma Separated Values) 파일로부터 DataFrame을 생성할 수 있습니다.\npd.read_csv() 함수를 사용합니다.\n\n\n\n    df = pd.read_csv('data.csv') # data.csv 파일로부터 DataFrame 생성\n    print(df)\n\n      Name  Age  Gender\n0    Alice   25  Female\n1      Bob   30    Male\n2  Charlie   28    Male\n\n\n\n\n\n\n\n\n\n\n생성 방법\n설명\n예시\n\n\n\n\n리스트 생성\n2차원 리스트를 사용하여 생성\npd.DataFrame(data, columns=['Name', 'Age', 'Gender'])\n\n\n딕셔너리 생성\n딕셔너리를 사용하여 생성\npd.DataFrame(data)\n\n\nCSV 파일 생성\nCSV 파일로부터 생성\npd.read_csv('data.csv')\n\n\n\n\n\n2.1.3 2.1.3 DataFrame 정보 확인\nDataFrame의 구조, 데이터 유형, 결측치 정보 등을 확인하는 것은 데이터 분석의 첫 단계입니다.\n\n(시각 자료 예시: DataFrame 정보 확인 방법을 보여주는 그림. head(), info(), describe() 등의 함수를 사용하여 DataFrame의 일부 데이터, 구조, 통계량 등을 확인하는 과정을 간략하게 보여주는 그림)\nhead():\n\nDataFrame의 처음 몇 행을 표시합니다.\n기본적으로 처음 5행을 표시하며, 괄호 안에 숫자를 넣어 표시할 행 수를 지정할 수 있습니다.\n\n\n\n    print(df.head()) # 처음 5행 표시\n    print(df.head(10)) # 처음 10행 표시\n\n      Name  Age  Gender\n0    Alice   25  Female\n1      Bob   30    Male\n2  Charlie   28    Male\n      Name  Age  Gender\n0    Alice   25  Female\n1      Bob   30    Male\n2  Charlie   28    Male\n\n\n\ninfo():\n\nDataFrame의 구조, 열 이름, 데이터 유형, 결측치 개수 등을 요약하여 표시합니다.\n\n\n\n    print(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    3 non-null      object\n 1   Age     3 non-null      int64 \n 2   Gender  3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\nNone\n\n\n\ndescribe():\n\n수치형 열에 대한 기술 통계량(평균, 표준편차, 최소값, 최대값, 사분위수 등)을 요약하여 표시합니다.\n\n\n\n    print(df.describe())\n\n             Age\ncount   3.000000\nmean   27.666667\nstd     2.516611\nmin    25.000000\n25%    26.500000\n50%    28.000000\n75%    29.000000\nmax    30.000000\n\n\n\n\n\n\n\n\n\n\n확인 함수\n설명\n예시\n\n\n\n\nhead()\nDataFrame의 처음 몇 행 표시\ndf.head()\n\n\ninfo()\nDataFrame의 구조, 데이터 유형, 결측치 정보 확인\ndf.info()\n\n\ndescribe()\n수치형 열의 기술 통계량 요약\ndf.describe()\n\n\n\n\n\n2.1.4 2.1.4 DataFrame 인덱싱 및 슬라이싱\nDataFrame에서 특정 데이터에 접근하거나, 특정 행 또는 열을 선택하는 것을 인덱싱(Indexing) 및 슬라이싱(Slicing)이라고 합니다.\n\n(시각 자료 예시: DataFrame 인덱싱 및 슬라이싱 방법을 보여주는 그림. [], loc[], iloc[] 등을 사용하여 특정 데이터에 접근하거나, 행/열을 선택하는 과정을 간략하게 보여주는 그림)\n[] 연산자:\n\n열 이름을 사용하여 열을 선택합니다.\n\n\n\n    print(df['Name']) # 'Name' 열 선택\n    print(df[['Name', 'Age']]) # 'Name'과 'Age' 열 선택\n\n0      Alice\n1        Bob\n2    Charlie\nName: Name, dtype: object\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   28\n\n\n\nloc[] 메서드:\n\n행과 열의 레이블을 사용하여 데이터에 접근합니다.\n\n\n\n    print(df.loc[0]) # 0번 행 선택\n    print(df.loc[0:2, 'Name']) # 0번부터 2번 행까지, 'Name' 열 선택\n    print(df.loc[:, ['Name', 'Age']]) # 모든 행, 'Name'과 'Age' 열 선택\n\nName       Alice\nAge           25\nGender    Female\nName: 0, dtype: object\n0      Alice\n1        Bob\n2    Charlie\nName: Name, dtype: object\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   28\n\n\n\niloc[] 메서드:\n\n행과 열의 정수 인덱스를 사용하여 데이터에 접근합니다.\n\n\n\n    print(df.iloc[0]) # 0번 행 선택\n    print(df.iloc[0:2, 0]) # 0번부터 2번 행까지, 0번 열 선택\n    print(df.iloc[:, [0, 1]]) # 모든 행, 0번과 1번 열 선택\n\nName       Alice\nAge           25\nGender    Female\nName: 0, dtype: object\n0    Alice\n1      Bob\nName: Name, dtype: object\n      Name  Age\n0    Alice   25\n1      Bob   30\n2  Charlie   28\n\n\n\n\n\n\n\n\n\n\n인덱싱/슬라이싱 방법\n설명\n예시\n\n\n\n\n[]\n열 이름으로 열 선택\ndf['Name']\n\n\nloc[]\n행/열 레이블로 데이터 접근\ndf.loc[0:2, 'Name']\n\n\niloc[]\n행/열 정수 인덱스로 데이터 접근\ndf.iloc[:, [0, 1]]\n\n\n\n\n\n2.1.5 2.1.5 DataFrame 데이터 조작\nDataFrame은 다양한 데이터 조작 기능을 제공합니다.\n\n(시각 자료 예시: DataFrame 데이터 조작 방법을 보여주는 그림. 새로운 열 추가, 열 삭제, 데이터 필터링, 정렬 등을 수행하는 과정을 간략하게 보여주는 그림)\n새로운 열 추가:\n\n\n    df['Salary'] = [50000, 60000, 55000] # 'Salary' 열 추가\n    df['Age_Plus_10'] = df['Age'] + 10 # 'Age' 열을 이용하여 'Age_Plus_10' 열 추가\n\n\n데이터 필터링:\n\n\n    df_filtered = df[df['Age'] &gt; 28] # 'Age' 열이 28보다 큰 행만 선택\n    df_filtered = df[(df['Age'] &gt; 25) & (df['Gender'] == 'Male')] # 'Age' 열이 25보다 크고 'Gender' 열이 'Male'인 행만 선택\n\n\n데이터 정렬:\n\n\n    df_sorted = df.sort_values(by='Age', ascending=True) # 'Age' 열을 기준으로 오름차순 정렬\n    df_sorted = df.sort_values(by=['Gender', 'Age'], ascending=[True, False]) # 'Gender' 열을 기준으로 오름차순 정렬 후, 'Age' 열을 기준으로 내림차순 정렬\n\n\n\n\n\n\n\n\n\n조작 방법\n설명\n예시\n\n\n\n\n새로운 열 추가\nDataFrame에 새로운 열 추가\ndf['Salary'] = [50000, 60000, 55000]\n\n\n열 삭제\nDataFrame에서 열 삭제\ndf = df.drop(columns=['Gender'])\n\n\n데이터 필터링\n특정 조건을 만족하는 행만 선택\ndf[df['Age'] &gt; 28]\n\n\n데이터 정렬\nDataFrame의 행을 특정 열을 기준으로 정렬\ndf.sort_values(by='Age')\n\n\n\n\n열 삭제:\n\n\n    df = df.drop(columns=['Gender']) # 'Gender' 열 삭제\n\n\n\n2.1.6 2.1.6 DataFrame 활용의 중요성\nPandas DataFrame은 사회과학 연구 데이터를 효율적으로 관리하고 분석하는 데 필수적인 도구입니다. DataFrame을 통해 데이터를 구조화하고, 다양한 데이터 조작 기능을 활용함으로써, 연구자는 데이터 분석 과정을 체계적으로 수행하고, 연구 결과의 신뢰성을 높일 수 있습니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초</span>"
    ]
  },
  {
    "objectID": "02-data-handling-exploration.html#파이썬을-이용한-데이터-핸들링-및-탐색-데이터-전처리",
    "href": "02-data-handling-exploration.html#파이썬을-이용한-데이터-핸들링-및-탐색-데이터-전처리",
    "title": "2  파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초",
    "section": "2.2 2.2 파이썬을 이용한 데이터 핸들링 및 탐색: 데이터 전처리",
    "text": "2.2 2.2 파이썬을 이용한 데이터 핸들링 및 탐색: 데이터 전처리\n사회과학 연구에서 수집된 데이터는 종종 결측치, 이상치, 불일치 등 다양한 문제점을 포함하고 있습니다. 이러한 데이터 문제를 해결하고 분석에 적합한 형태로 데이터를 변환하는 과정을 데이터 전처리(Data Preprocessing)라고 합니다. 본 절에서는 파이썬 Pandas 라이브러리를 이용하여 데이터 전처리를 수행하는 주요 방법에 대해 설명합니다.\n\n(시각 자료 예시: 데이터 전처리 과정을 보여주는 흐름도. 결측치 처리, 이상치 처리, 변수 변환 등의 단계를 포함하는 그림)\n\n\n2.2.1 2.2.1 결측치 처리\n결측치(Missing Data)는 데이터에 값이 존재하지 않는 경우를 의미합니다. 결측치는 데이터 분석 결과의 왜곡을 초래할 수 있으므로, 적절한 방법으로 처리해야 합니다.\n\n(시각 자료 예시: 결측치 처리 방법을 비교하는 표. 삭제, 대체, 예측 등 다양한 방법의 장단점을 비교하는 표)\n\n결측치 확인:\n\nisnull() 또는 isna() 함수를 사용하여 결측치를 확인할 수 있습니다.\nsum() 함수와 함께 사용하여 결측치의 개수를 확인할 수 있습니다.\n\n\n\n\n    import pandas as pd\n\n    # 예시 DataFrame 생성\n    data = {'A': [1, 2, None, 4, 5], 'B': [6, None, 8, 9, 10], 'C': ['a', 'b', 'c', None, 'e']}\n    df = pd.DataFrame(data)\n\n    print(df.isnull())\n    print(df.isnull().sum())\n\n       A      B      C\n0  False  False  False\n1  False   True  False\n2   True  False  False\n3  False  False   True\n4  False  False  False\nA    1\nB    1\nC    1\ndtype: int64\n\n\n* **결측치 삭제:**\n    * `dropna()` 함수를 사용하여 결측치를 포함하는 행 또는 열을 삭제할 수 있습니다.\n    * `axis` 매개변수를 사용하여 행(0) 또는 열(1) 삭제를 지정할 수 있습니다.\n    * `how` 매개변수를 사용하여 모든 값이 결측치인 경우(`'all'`) 또는 하나라도 결측치가 있는 경우(`'any'`)를 지정할 수 있습니다.\n\n    df_dropped_row = df.dropna(axis=0)\n    df_dropped_col = df.dropna(axis=1)\n    df_dropped_all = df.dropna(axis=0, how='all')\n    print(df_dropped_row)\n    print(df_dropped_col)\n    print(df_dropped_all)\n\n     A     B  C\n0  1.0   6.0  a\n4  5.0  10.0  e\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4]\n     A     B     C\n0  1.0   6.0     a\n1  2.0   NaN     b\n2  NaN   8.0     c\n3  4.0   9.0  None\n4  5.0  10.0     e\n\n\n* **결측치 대체:**\n    * `fillna()` 함수를 사용하여 결측치를 특정 값으로 대체할 수 있습니다.\n    * `value` 매개변수를 사용하여 대체할 값을 지정할 수 있습니다.\n    * `method` 매개변수를 사용하여 이전 값(`'ffill'`) 또는 다음 값(`'bfill'`)으로 대체할 수 있습니다.\n    * `mean()`, `median()`, `mode()` 등 통계 함수를 사용하여 대체값을 계산할 수 있습니다.\n\n    df_filled_0 = df.fillna(0)\n    df_filled_mean = df.fillna(df.mean(numeric_only=True))\n    df_filled_ffill = df.fillna(method='ffill')\n    print(df_filled_0)\n    print(df_filled_mean)\n    print(df_filled_ffill)\n\n     A     B  C\n0  1.0   6.0  a\n1  2.0   0.0  b\n2  0.0   8.0  c\n3  4.0   9.0  0\n4  5.0  10.0  e\n     A      B     C\n0  1.0   6.00     a\n1  2.0   8.25     b\n2  3.0   8.00     c\n3  4.0   9.00  None\n4  5.0  10.00     e\n     A     B  C\n0  1.0   6.0  a\n1  2.0   6.0  b\n2  2.0   8.0  c\n3  4.0   9.0  c\n4  5.0  10.0  e\n\n\nC:\\Users\\voidm\\AppData\\Local\\Temp\\ipykernel_6684\\912558651.py:3: FutureWarning:\n\nDataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n\n\n\n|   처리 방법   |                       설명                       |                       예시                       |\n| :-----------: | :--------------------------------------------: | :------------------------------------------------: |\n|   결측치 확인   |               `isnull()`, `isna()`               |                   `df.isnull()`                   |\n|   결측치 삭제   |                  `dropna()`                  |                 `df.dropna()`                 |\n|   결측치 대체   |                  `fillna()`                  |                 `df.fillna(0)`                 |\n\n\n2.2.2 2.2.2 이상치 처리\n이상치(Outlier)는 다른 데이터와 확연히 구분되는 극단적인 값을 의미합니다. 이상치는 데이터 분석 결과의 왜곡을 초래할 수 있으므로, 탐지하고 적절하게 처리해야 합니다.\n\n(시각 자료 예시: 이상치 탐지 방법을 보여주는 그림. 상자 그림, 산점도 등을 사용하여 이상치를 시각적으로 탐지하는 그림)\n\n이상치 탐지:\n\n시각적 방법: 상자 그림(boxplot()), 산점도(scatter()) 등을 사용하여 이상치를 시각적으로 탐지할 수 있습니다.\n\n\n\n\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.boxplot(x=df['B'])\n    plt.show()\n\n    sns.scatterplot(x=df.index, y=df['B'])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    * **통계적 방법:** Z-점수, 사분위 범위(Interquartile Range, IQR) 등을 사용하여 이상치를 탐지할 수 있습니다.\n\n    Q1 = df['B'].quantile(0.25)\n    Q3 = df['B'].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = df[(df['B'] &lt; lower_bound) | (df['B'] &gt; upper_bound)]\n    print(outliers)\n\nEmpty DataFrame\nColumns: [A, B, C]\nIndex: []\n\n\n* **이상치 처리:**\n    * **삭제:** 이상치를 포함하는 행 또는 열을 삭제합니다.\n    * **대체:** 이상치를 특정 값(평균, 중앙값, 최빈값 등)으로 대체하거나, 상한/하한 값으로 제한합니다.\n    * **변환:** 로그 변환, 제곱근 변환 등을 통해 이상치의 영향을 줄입니다.\n\n    df_clipped = df.copy()\n    df_clipped['B'] = df_clipped['B'].clip(lower=lower_bound, upper=upper_bound)\n    print(df_clipped)\n\n     A     B     C\n0  1.0   6.0     a\n1  2.0   NaN     b\n2  NaN   8.0     c\n3  4.0   9.0  None\n4  5.0  10.0     e\n\n\n|   처리 방법   |                       설명                       |                       예시                       |\n| :-----------: | :--------------------------------------------: | :------------------------------------------------: |\n|   이상치 탐지   |         시각적/통계적 방법으로 이상치 확인         |                   상자 그림, IQR                   |\n|     삭제     |                 이상치를 포함하는 행/열 삭제                 |                 `df.dropna()`                 |\n|     대체     |            이상치를 특정 값으로 대체/제한            |                   `df.fillna()`                   |\n|     변환     |                로그 변환, 제곱근 변환 등                |                  `np.log()`                  |\n\n\n2.2.3 2.2.3 변수 변환\n변수 변환(Variable Transformation)은 변수의 형태나 분포를 변경하여 데이터 분석의 효율성을 높이거나, 특정 분석 방법에 적합하도록 데이터를 만드는 과정입니다.\n\n(시각 자료 예시: 변수 변환 방법을 보여주는 그림. 로그 변환, 표준화, 정규화 등을 통해 변수의 분포나 척도를 변경하는 그림)\n\n척도 변환(Scaling):\n\n변수의 척도를 변경하여 서로 다른 변수 간의 비교를 용이하게 하거나, 특정 알고리즘의 성능을 향상시킵니다.\n표준화(Standardization): 변수의 평균을 0, 표준편차를 1로 변환합니다.\n\n\\(Z = \\frac{X - \\mu}{\\sigma}\\)\n\n\n\n    from sklearn.preprocessing import StandardScaler\n\n    scaler = StandardScaler()\n    df['A_scaled'] = scaler.fit_transform(df[['A']])\n    print(df[['A', 'A_scaled']].head())\n\n     A  A_scaled\n0  1.0 -1.264911\n1  2.0 -0.632456\n2  NaN       NaN\n3  4.0  0.632456\n4  5.0  1.264911\n\n\n    * **정규화(Normalization):** 변수의 값을 0과 1 사이의 범위로 변환합니다.\n\n    $X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$\n\n    from sklearn.preprocessing import MinMaxScaler\n\n    scaler = MinMaxScaler()\n    df['B_scaled'] = scaler.fit_transform(df[['B']])\n    print(df[['B', 'B_scaled']].head())\n\n      B  B_scaled\n0   6.0      0.00\n1   NaN       NaN\n2   8.0      0.50\n3   9.0      0.75\n4  10.0      1.00\n\n\n* **분포 변환(Distribution Transformation):**\n    * 변수의 분포를 변경하여 특정 분석 방법에 적합하도록 만듭니다.\n    * **로그 변환(Log Transformation):** 오른쪽으로 꼬리가 긴 분포(Right-Skewed Distribution)를 정규 분포에 가깝게 만듭니다.\n\n    import numpy as np\n\n    df_log = df.copy()\n    df_log['A_log'] = np.log(df_log['A'][df_log['A'] &gt; 0]) # 0 또는 음수 값을 제외하고 로그 변환\n    print(df_log[['A', 'A_log']].head())\n\n     A     A_log\n0  1.0  0.000000\n1  2.0  0.693147\n2  NaN       NaN\n3  4.0  1.386294\n4  5.0  1.609438\n\n\n    * **제곱근 변환(Square Root Transformation):** 로그 변환과 유사한 효과를 가지며, 0을 포함하는 데이터에 적용할 수 있습니다.\n\n    df_sqrt = df.copy()\n    df_sqrt['B_sqrt'] = np.sqrt(df_sqrt['B'][df_sqrt['B'] &gt;= 0]) # 음수 값을 제외하고 제곱근 변환\n    print(df_sqrt[['B', 'B_sqrt']].head())\n\n      B    B_sqrt\n0   6.0  2.449490\n1   NaN       NaN\n2   8.0  2.828427\n3   9.0  3.000000\n4  10.0  3.162278\n\n\n* **범주형 변수 처리:**\n    * 범주형 변수를 분석에 활용하기 위해 수치형 변수로 변환합니다.\n    * **더미 변수화(Dummy Variable Encoding):** 각 범주를 0 또는 1로 표현하는 새로운 변수를 생성합니다.\n\n    df_encoded = pd.get_dummies(df, columns=['C'], drop_first=True) # 'C' 열을 더미 변수화 (첫 번째 범주 제외)\n    print(df_encoded.head())\n\n     A     B  A_scaled  B_scaled    C_b    C_c    C_e\n0  1.0   6.0 -1.264911      0.00  False  False  False\n1  2.0   NaN -0.632456       NaN   True  False  False\n2  NaN   8.0       NaN      0.50  False   True  False\n3  4.0   9.0  0.632456      0.75  False  False  False\n4  5.0  10.0  1.264911      1.00  False  False   True\n\n\n    * **원-핫 인코딩(One-Hot Encoding):** 더미 변수화와 유사하지만, 첫 번째 범주를 제외하지 않습니다.\n\n    df_onehot = pd.get_dummies(df, columns=['C']) # 'C' 열을 원-핫 인코딩\n    print(df_onehot.head())\n\n     A     B  A_scaled  B_scaled    C_a    C_b    C_c    C_e\n0  1.0   6.0 -1.264911      0.00   True  False  False  False\n1  2.0   NaN -0.632456       NaN  False   True  False  False\n2  NaN   8.0       NaN      0.50  False  False   True  False\n3  4.0   9.0  0.632456      0.75  False  False  False  False\n4  5.0  10.0  1.264911      1.00  False  False  False   True\n\n\n|   변환 방법   |                       설명                       |                       예시                       |\n| :-----------: | :--------------------------------------------: | :------------------------------------------------: |\n|   척도 변환   |               변수의 척도를 변경               |         표준화, 정규화         |\n|   분포 변환   |               변수의 분포를 변경               |         로그 변환, 제곱근 변환         |\n| 범주형 변수 처리 |             범주형 변수를 수치형으로 변환             | 더미 변수화, 원-핫 인코딩 |\n\n\n2.2.4 2.2.4 데이터 전처리 과정의 중요성\n데이터 전처리는 데이터 분석의 정확성과 신뢰성을 높이는 데 필수적인 과정입니다. 적절한 전처리 과정을 통해 데이터의 품질을 향상시키고, 분석에 적합한 형태로 데이터를 변환함으로써, 연구자는 더욱 정확하고 의미 있는 결과를 얻을 수 있습니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초</span>"
    ]
  },
  {
    "objectID": "02-data-handling-exploration.html#sec-descriptive-stats",
    "href": "02-data-handling-exploration.html#sec-descriptive-stats",
    "title": "2  파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초",
    "section": "2.3 2.3 기술 통계 분석",
    "text": "2.3 2.3 기술 통계 분석\n데이터 전처리를 통해 데이터를 분석에 적합한 형태로 정제했다면, 다음 단계는 데이터의 기본적인 특성을 파악하는 것입니다. 기술 통계 분석(Descriptive Statistics Analysis)은 수집된 데이터를 요약하고 설명하여 데이터가 가진 정보를 효과적으로 전달하는 통계적 방법입니다. 이는 데이터의 중심 경향성(Central Tendency), 산포도(Dispersion), 분포 형태(Distribution Shape) 등을 파악하여 데이터에 대한 깊이 있는 이해를 돕고, 이어질 추론 통계 분석의 기초를 마련합니다.\n본 절에서는 파이썬 Pandas 라이브러리를 활용하여 주요 기술 통계량을 계산하고 해석하는 방법을 학습합니다.\n\n\n\n\n\n\n📌 핵심 요약\n\n\n\n기술 통계 분석은 데이터의 기본적인 특성을 요약하고 설명하는 과정입니다. Pandas를 이용하여 평균, 중앙값, 최빈값 등 중심 경향성 지표와 분산, 표준편차, 범위, 사분위수 등 산포도 지표를 계산하고 해석하는 방법을 배웁니다. 또한, 범주형 데이터의 빈도 분석 방법과 .describe() 함수를 활용한 요약 통계량 확인 방법을 익힙니다.\n\n\n(시각 자료 예시: 기술 통계 분석의 주요 목표(중심 경향성, 산포도, 분포 형태 파악)를 보여주는 다이어그램)\n\n2.3.1 가상 데이터 생성\n기술 통계 분석 실습을 위해 가상의 설문조사 응답 데이터를 담은 Pandas DataFrame을 생성해 보겠습니다. 이 데이터는 ‘ID’, ‘Age’, ‘Gender’, ‘EducationLevel’, ‘SatisfactionScore’, ‘Income’ 열을 포함합니다.\n\nimport pandas as pd\nimport numpy as np\n\n# 가상 데이터 생성\ndata = {\n    'ID': range(1, 11),\n    'Age': [25, 30, 28, 35, 42, 29, 31, 45, 22, 38],\n    'Gender': ['Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male'],\n    'EducationLevel': ['Bachelor', 'Master', 'Bachelor', 'PhD', 'Master', 'Bachelor', 'Master', 'PhD', 'High School', 'Master'],\n    'SatisfactionScore': [7, 8, 6, 9, 7, np.nan, 8, 10, 5, 7], # 결측치(NaN) 포함\n    'Income': [5000, 6000, 5500, 7500, 8000, 5200, 6100, 9000, 4000, 7000]\n}\ndf_survey = pd.DataFrame(data)\n\nprint(\"가상 설문조사 데이터:\")\nprint(df_survey.head()) # 데이터 일부 확인\nprint(\"\\n데이터 정보 요약:\")\nprint(df_survey.info()) # 데이터 구조 및 결측치 확인\n\n가상 설문조사 데이터:\n   ID  Age  Gender EducationLevel  SatisfactionScore  Income\n0   1   25  Female       Bachelor                7.0    5000\n1   2   30    Male         Master                8.0    6000\n2   3   28    Male       Bachelor                6.0    5500\n3   4   35  Female            PhD                9.0    7500\n4   5   42    Male         Master                7.0    8000\n\n데이터 정보 요약:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   ID                 10 non-null     int64  \n 1   Age                10 non-null     int64  \n 2   Gender             10 non-null     object \n 3   EducationLevel     10 non-null     object \n 4   SatisfactionScore  9 non-null      float64\n 5   Income             10 non-null     int64  \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 612.0+ bytes\nNone\n\n\n참고: Pandas의 대부분 기술 통계 함수는 기본적으로 결측치(NaN)를 제외하고 계산합니다. 이는 skipna=True (기본값) 설정 때문입니다.\n\n\n2.3.2 2.3.1 중심 경향성 측정 (Measures of Central Tendency)\n데이터의 중심 경향성은 데이터 값들이 어떤 값을 중심으로 분포하는 경향이 있는지를 나타내는 척도입니다. 대표적인 중심 경향성 측정 지표로는 평균, 중앙값, 최빈값이 있습니다.\n\n(시각 자료 예시: 정규분포 곡선 위에 평균, 중앙값, 최빈값이 일치하는 모습을 보여주고, 비대칭 분포(skewed distribution)에서 세 값이 달라지는 모습을 비교하는 그림)\n평균 (Mean):\n\n모든 데이터 값을 더한 후 데이터 개수로 나눈 값입니다.\n데이터 전체의 값을 반영하지만, 극단적인 값(이상치)에 영향을 받기 쉽습니다.\nPandas에서는 .mean() 메서드를 사용합니다.\n\n\n\n    # Age와 Income 열의 평균 계산\n    mean_age = df_survey['Age'].mean()\n    mean_income = df_survey['Income'].mean()\n    mean_satisfaction = df_survey['SatisfactionScore'].mean() # NaN은 자동 제외\n\n    print(f\"평균 나이: {mean_age:.2f}\")\n    print(f\"평균 소득: {mean_income:.2f}\")\n    print(f\"평균 만족도 점수: {mean_satisfaction:.2f}\")\n\n평균 나이: 32.50\n평균 소득: 6330.00\n평균 만족도 점수: 7.44\n\n\n\n중앙값 (Median):\n\n데이터를 크기 순서대로 정렬했을 때 중앙에 위치하는 값입니다.\n데이터 개수가 짝수일 경우, 중앙에 위치한 두 값의 평균을 사용합니다.\n평균과 달리 극단적인 값(이상치)에 영향을 덜 받습니다 (Robust).\nPandas에서는 .median() 메서드를 사용합니다.\n\n\n\n    # Age와 Income 열의 중앙값 계산\n    median_age = df_survey['Age'].median()\n    median_income = df_survey['Income'].median()\n    median_satisfaction = df_survey['SatisfactionScore'].median() # NaN은 자동 제외\n\n    print(f\"나이 중앙값: {median_age}\")\n    print(f\"소득 중앙값: {median_income}\")\n    print(f\"만족도 점수 중앙값: {median_satisfaction}\")\n\n    # 평균과 중앙값 비교 (소득 분포가 비대칭적일 경우 차이가 발생할 수 있음)\n    print(f\"\\n소득 - 평균: {mean_income:.2f}, 중앙값: {median_income}\")\n\n나이 중앙값: 30.5\n소득 중앙값: 6050.0\n만족도 점수 중앙값: 7.0\n\n소득 - 평균: 6330.00, 중앙값: 6050.0\n\n\n\n최빈값 (Mode):\n\n데이터에서 가장 빈번하게 나타나는 값입니다.\n연속형 데이터보다는 범주형 데이터나 이산형 데이터에서 주로 사용됩니다.\n최빈값은 여러 개 존재할 수 있습니다.\nPandas에서는 .mode() 메서드를 사용합니다. .mode()는 최빈값이 여러 개일 경우 모든 값을 Series 형태로 반환합니다.\n\n\n\n    # Gender와 EducationLevel 열의 최빈값 계산\n    mode_gender = df_survey['Gender'].mode()\n    mode_education = df_survey['EducationLevel'].mode()\n    mode_satisfaction = df_survey['SatisfactionScore'].mode() # 최빈값은 여러 개일 수 있음\n\n    print(f\"성별 최빈값:\\n{mode_gender}\")\n    print(f\"\\n학력 최빈값:\\n{mode_education}\")\n    print(f\"\\n만족도 점수 최빈값:\\n{mode_satisfaction}\") # 7점이 3번으로 가장 많음\n\n성별 최빈값:\n0    Female\n1      Male\nName: Gender, dtype: object\n\n학력 최빈값:\n0    Master\nName: EducationLevel, dtype: object\n\n만족도 점수 최빈값:\n0    7.0\nName: SatisfactionScore, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n측정 지표\n설명\nPandas 메서드\n특징\n\n\n\n\n평균 (Mean)\n모든 값의 합 / 개수\n.mean()\n이상치에 민감\n\n\n중앙값 (Median)\n정렬 시 중앙에 위치하는 값\n.median()\n이상치에 덜 민감 (Robust)\n\n\n최빈값 (Mode)\n가장 빈번하게 나타나는 값\n.mode()\n범주형 데이터에 유용, 여러 개 가능\n\n\n\n\n\n2.3.3 2.3.2 산포도 측정 (Measures of Dispersion)\n산포도는 데이터 값들이 중심 경향성 값(주로 평균)으로부터 얼마나 흩어져 있는지를 나타내는 척도입니다. 데이터의 변동성이나 다양성을 파악하는 데 중요합니다.\n\n(시각 자료 예시: 평균은 같지만 분산(표준편차)이 다른 두 분포를 비교하는 그림. 좁고 뾰족한 분포와 넓고 평평한 분포)\n범위 (Range):\n\n데이터의 최댓값과 최솟값의 차이입니다.\n계산이 간단하지만, 양 극단의 값에만 의존하므로 데이터 전체의 흩어짐 정도를 충분히 반영하지 못할 수 있습니다.\nPandas에서는 .max()와 .min()을 이용하여 계산합니다.\n\n\n\n    # Age의 범위 계산\n    range_age = df_survey['Age'].max() - df_survey['Age'].min()\n    print(f\"나이 범위: {range_age} (최소: {df_survey['Age'].min()}, 최대: {df_survey['Age'].max()})\")\n\n나이 범위: 23 (최소: 22, 최대: 45)\n\n\n\n분산 (Variance) 및 표준편차 (Standard Deviation):\n\n분산: 각 데이터 값이 평균으로부터 얼마나 떨어져 있는지(편차)를 제곱하여 평균 낸 값입니다. 데이터의 퍼짐 정도를 나타냅니다. 단위가 원래 데이터 단위의 제곱이 됩니다.\n표준편차: 분산에 제곱근을 취한 값입니다. 분산과 동일하게 데이터의 퍼짐 정도를 나타내지만, 단위가 원래 데이터와 동일하여 해석이 용이합니다.\nPandas에서는 .var() (분산)와 .std() (표준편차) 메서드를 사용합니다. 기본적으로 표본 분산/표준편차 (n-1로 나눔, ddof=1)를 계산합니다.\n\n\n\n    # Age와 Income의 분산 및 표준편차 계산\n    var_age = df_survey['Age'].var()\n    std_age = df_survey['Age'].std()\n    var_income = df_survey['Income'].var()\n    std_income = df_survey['Income'].std()\n    std_satisfaction = df_survey['SatisfactionScore'].std()\n\n    print(f\"나이 분산: {var_age:.2f}, 표준편차: {std_age:.2f}\")\n    print(f\"소득 분산: {var_income:.2f}, 표준편차: {std_income:.2f}\")\n    print(f\"만족도 점수 표준편차: {std_satisfaction:.2f}\")\n\n나이 분산: 54.50, 표준편차: 7.38\n소득 분산: 2340111.11, 표준편차: 1529.74\n만족도 점수 표준편차: 1.51\n\n\n\n사분위수 (Quartiles) 및 사분위 범위 (Interquartile Range, IQR):\n\n사분위수: 데이터를 크기 순으로 정렬했을 때, 4등분하는 위치에 있는 값입니다.\n\n제1사분위수 (Q1): 하위 25% 지점의 값\n제2사분위수 (Q2): 50% 지점의 값 (중앙값과 동일)\n제3사분위수 (Q3): 하위 75% 지점의 값\n\n사분위 범위 (IQR): 제3사분위수(Q3)와 제1사분위수(Q1)의 차이 (IQR = Q3 - Q1)입니다. 데이터의 중간 50%가 분포하는 범위이며, 이상치 탐지에도 활용됩니다 (2.2.2절 참고).\nPandas에서는 .quantile() 메서드를 사용하여 특정 분위수를 계산할 수 있습니다. (예: df['col'].quantile(0.25)는 Q1)\n\n\n\n    # Income의 사분위수 및 IQR 계산\n    Q1_income = df_survey['Income'].quantile(0.25)\n    Q2_income = df_survey['Income'].quantile(0.50) # 중앙값과 동일\n    Q3_income = df_survey['Income'].quantile(0.75)\n    IQR_income = Q3_income - Q1_income\n\n    print(f\"소득 제1사분위수 (Q1): {Q1_income}\")\n    print(f\"소득 제2사분위수 (Q2/Median): {Q2_income}\")\n    print(f\"소득 제3사분위수 (Q3): {Q3_income}\")\n    print(f\"소득 사분위 범위 (IQR): {IQR_income}\")\n\n소득 제1사분위수 (Q1): 5275.0\n소득 제2사분위수 (Q2/Median): 6050.0\n소득 제3사분위수 (Q3): 7375.0\n소득 사분위 범위 (IQR): 2100.0\n\n\n\n\n\n\n\n\n\n\n\n측정 지표\n설명\nPandas 메서드 / 계산\n특징\n\n\n\n\n범위 (Range)\n최댓값 - 최솟값\n.max() - .min()\n계산 간편, 이상치에 민감\n\n\n분산 (Variance)\n편차 제곱의 평균\n.var()\n퍼짐 정도 측정, 단위가 제곱됨\n\n\n표준편차 (Std Dev)\n분산의 제곱근\n.std()\n퍼짐 정도 측정, 원래 단위와 동일\n\n\n사분위수 (Quartile)\n데이터를 4등분하는 지점의 값\n.quantile()\n데이터 분포 위치 파악\n\n\nIQR\nQ3 - Q1\n.quantile(0.75) - .quantile(0.25)\n중앙 50% 데이터 범위, 이상치 영향 적음\n\n\n\n\n\n2.3.4 2.3.3 분포 형태 파악 (Understanding Distribution Shape)\n기술 통계는 데이터 분포의 형태에 대한 정보도 제공합니다. 왜도와 첨도는 분포의 비대칭성과 뾰족한 정도를 나타내는 지표입니다.\n\n(시각 자료 예시: 좌측으로 치우친 분포, 우측으로 치우친 분포, 대칭 분포(정규분포)의 히스토그램과 함께 왜도 값을 보여주는 그림)\n(시각 자료 예시: 뾰족한 분포(첨도 &gt; 0), 정규분포(첨도 = 0), 평평한 분포(첨도 &lt; 0)의 히스토그램과 함께 첨도 값을 보여주는 그림)\n왜도 (Skewness):\n\n분포의 비대칭 정도를 나타냅니다.\n왜도 = 0: 좌우 대칭 (예: 정규분포)\n왜도 &gt; 0: 오른쪽 꼬리가 긴 분포 (Right-skewed, Positive skew) - 평균 &gt; 중앙값\n왜도 &lt; 0: 왼쪽 꼬리가 긴 분포 (Left-skewed, Negative skew) - 평균 &lt; 중앙값\nPandas에서는 .skew() 메서드를 사용합니다.\n\n첨도 (Kurtosis):\n\n분포의 뾰족한 정도와 꼬리 두께를 나타냅니다 (정규분포 대비).\n첨도 = 0: 정규분포와 유사한 뾰족함 (Mesokurtic)\n첨도 &gt; 0: 정규분포보다 더 뾰족하고 꼬리가 두꺼운 분포 (Leptokurtic)\n첨도 &lt; 0: 정규분포보다 더 평평하고 꼬리가 얇은 분포 (Platykurtic)\nPandas에서는 .kurt() 또는 .kurtosis() 메서드를 사용합니다.\n\n\n\n    # Age와 Income의 왜도 및 첨도 계산\n    skew_age = df_survey['Age'].skew()\n    kurt_age = df_survey['Age'].kurt()\n    skew_income = df_survey['Income'].skew()\n    kurt_income = df_survey['Income'].kurt()\n\n    print(f\"나이 왜도: {skew_age:.2f}, 첨도: {kurt_age:.2f}\")\n    print(f\"소득 왜도: {skew_income:.2f}, 첨도: {kurt_income:.2f}\")\n\n나이 왜도: 0.43, 첨도: -0.71\n소득 왜도: 0.33, 첨도: -0.54\n\n\n**참고:** 왜도와 첨도는 값 자체만으로 분포 형태를 단정하기 어려울 수 있으며, 히스토그램 등 시각화(2.4절에서 다룸)를 통해 함께 확인하는 것이 좋습니다.\n\n\n2.3.5 2.3.4 범주형 데이터 기술 통계 (Descriptive Statistics for Categorical Data)\n범주형 데이터(성별, 학력 등)는 평균이나 표준편차 같은 수치적 요약보다는 각 범주의 빈도(Frequency)나 비율(Proportion)을 파악하는 것이 중요합니다.\n\n빈도표 (Frequency Table):\n\n각 범주에 속하는 데이터의 개수(빈도)를 나타내는 표입니다.\nPandas에서는 .value_counts() 메서드를 사용하여 쉽게 생성할 수 있습니다.\n\n\n\n    # Gender와 EducationLevel의 빈도표 생성\n    freq_gender = df_survey['Gender'].value_counts()\n    freq_education = df_survey['EducationLevel'].value_counts()\n\n    print(\"성별 빈도:\")\n    print(freq_gender)\n    print(\"\\n학력 빈도:\")\n    print(freq_education)\n\n성별 빈도:\nGender\nFemale    5\nMale      5\nName: count, dtype: int64\n\n학력 빈도:\nEducationLevel\nMaster         4\nBachelor       3\nPhD            2\nHigh School    1\nName: count, dtype: int64\n\n\n\n비율 (Proportions):\n\n각 범주의 빈도를 전체 데이터 개수로 나눈 값입니다. 전체에서 각 범주가 차지하는 비중을 백분율 등으로 나타낼 수 있습니다.\n.value_counts(normalize=True)를 사용하면 비율을 계산할 수 있습니다.\n\n\n\n    # Gender와 EducationLevel의 비율 계산\n    prop_gender = df_survey['Gender'].value_counts(normalize=True)\n    prop_education = df_survey['EducationLevel'].value_counts(normalize=True)\n\n    print(\"성별 비율:\")\n    print(prop_gender)\n    print(\"\\n학력 비율:\")\n    print(prop_education)\n\n    # 백분율로 표시\n    print(\"\\n학력 백분율:\")\n    print((prop_education * 100).round(2).astype(str) + '%')\n\n성별 비율:\nGender\nFemale    0.5\nMale      0.5\nName: proportion, dtype: float64\n\n학력 비율:\nEducationLevel\nMaster         0.4\nBachelor       0.3\nPhD            0.2\nHigh School    0.1\nName: proportion, dtype: float64\n\n학력 백분율:\nEducationLevel\nMaster         40.0%\nBachelor       30.0%\nPhD            20.0%\nHigh School    10.0%\nName: proportion, dtype: object\n\n\n\n\n\n\n\n\n\n\n분석 방법\n설명\nPandas 메서드\n\n\n\n\n빈도표 (Frequency)\n각 범주의 개수\n.value_counts()\n\n\n비율 (Proportion)\n각 범주의 비율 (전체 대비)\n.value_counts(normalize=True)\n\n\n\n\n\n2.3.6 2.3.5 모든 변수 요약: .describe() 메서드 활용\nPandas의 .describe() 메서드는 DataFrame의 수치형 변수에 대한 주요 기술 통계량(개수, 평균, 표준편차, 최소값, 사분위수, 최댓값)을 한 번에 계산하여 보여줍니다. 이는 데이터 탐색 초기 단계에서 매우 유용합니다.\n\n# 수치형 변수에 대한 기술 통계 요약\nnumeric_summary = df_survey.describe()\nprint(\"수치형 변수 요약 통계:\")\nprint(numeric_summary)\n\n수치형 변수 요약 통계:\n             ID        Age  SatisfactionScore       Income\ncount  10.00000  10.000000           9.000000    10.000000\nmean    5.50000  32.500000           7.444444  6330.000000\nstd     3.02765   7.382412           1.509231  1529.742171\nmin     1.00000  22.000000           5.000000  4000.000000\n25%     3.25000  28.250000           7.000000  5275.000000\n50%     5.50000  30.500000           7.000000  6050.000000\n75%     7.75000  37.250000           8.000000  7375.000000\nmax    10.00000  45.000000          10.000000  9000.000000\n\n\ninclude='all' 옵션을 사용하면 수치형 변수뿐만 아니라 범주형 변수에 대한 요약 정보(개수, 고유값 개수, 최빈값, 최빈값 빈도)도 함께 확인할 수 있습니다.\n\n# 모든 변수에 대한 기술 통계 요약 (범주형 포함)\nall_summary = df_survey.describe(include='all')\nprint(\"\\n모든 변수 요약 통계:\")\nprint(all_summary)\n\n\n모든 변수 요약 통계:\n              ID        Age  Gender EducationLevel  SatisfactionScore  \\\ncount   10.00000  10.000000      10             10           9.000000   \nunique       NaN        NaN       2              4                NaN   \ntop          NaN        NaN  Female         Master                NaN   \nfreq         NaN        NaN       5              4                NaN   \nmean     5.50000  32.500000     NaN            NaN           7.444444   \nstd      3.02765   7.382412     NaN            NaN           1.509231   \nmin      1.00000  22.000000     NaN            NaN           5.000000   \n25%      3.25000  28.250000     NaN            NaN           7.000000   \n50%      5.50000  30.500000     NaN            NaN           7.000000   \n75%      7.75000  37.250000     NaN            NaN           8.000000   \nmax     10.00000  45.000000     NaN            NaN          10.000000   \n\n             Income  \ncount     10.000000  \nunique          NaN  \ntop             NaN  \nfreq            NaN  \nmean    6330.000000  \nstd     1529.742171  \nmin     4000.000000  \n25%     5275.000000  \n50%     6050.000000  \n75%     7375.000000  \nmax     9000.000000  \n\n\n참고: .describe(include='all') 결과에서 수치형 변수의 ‘unique’, ‘top’, ‘freq’는 NaN으로 표시되고, 범주형 변수의 ’mean’, ‘std’, ‘min’, ‘25%’, ‘50%’, ‘75%’, ’max’는 NaN으로 표시됩니다. ’count’는 모든 변수 유형에 대해 유효한 값(non-null 개수)을 보여줍니다.\n\n\n2.3.7 2.3.6 기술 통계 분석의 중요성\n기술 통계 분석은 복잡한 데이터를 이해하기 쉬운 형태로 요약하고, 데이터의 숨겨진 패턴이나 특징을 발견하는 첫걸음입니다. 사회과학 연구에서 연구 가설을 설정하거나, 분석 방법을 선택하거나, 분석 결과를 해석하는 데 있어 데이터의 기본적인 특성을 파악하는 것은 필수적입니다. Pandas를 활용한 기술 통계 분석은 이러한 과정을 효율적이고 체계적으로 수행할 수 있도록 돕습니다. 다음 절에서는 이러한 데이터를 시각적으로 탐색하는 방법에 대해 알아보겠습니다.",
    "crumbs": [
      "제1부 - 파이썬을 활용한 사회과학 데이터 분석 준비",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>파이썬을 이용한 데이터 핸들링 및 탐색: Pandas DataFrame 기초</span>"
    ]
  }
]